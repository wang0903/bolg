---
cover: /assets/images/cover2.jpg
icon: pen-to-square
date: 2024-06-14
category:
  - K8S
tag:
  - k8s
  - 微服务
star: true
sticky: true #标记
---
# 用docker安装K8S

## 基础设置

### 1、设置主机名

```sh
hostnamectl set-hostname 主机名称
```

### 2、同步hosts文件

> 如果DNS不支持主机名称解析，还需要在每台机器上的/etc/hosts/文件中添加主机名和IP的对应关系

```sh
cat >> /etc/hosts <<EOF
192.168.127.145 master
192.168.127.146 word01
192.168.127.147 word02
EOF
```

### 3、防火墙关闭

```sh
sudo systemctl stop firewalld #关闭防火墙
sudo systemctl disable firewalld #防止启动开启防火墙
```

### 4、关闭 SELINUX

```sh
setenforce 0 && sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
```

### 5、关闭swap分区

```sh
swapoff -a && sed -ri 's/.*swap.*/#&/' /etc/fstab
```

### 6、同步时间，让每个节点在同一个时间下

```sh
yum install ntpdate -y #安装同步时间的工具
ntpdate time1.aliyun.com #进行时间
```

### 7、安装ipset、ipvsadm

```sh
yum -y install ipset ipvsadm
```

### 8、写入配置文件中

```sh
cat > /etc/sysconfig/modules/ipvs.modules <<EOF
# !/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
EOF
```

```sh
#授权、运行、检查是否加载
chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack
```

### 9、启动模块

```sh
systemctl enable --now systemd-modules-load.service
```

### 10、查看模块是否启动成功

```sh
lsmod |egrep "ip_vs|nf_conntrack_ipv4"
```

### 11、添加网桥过滤及内核转发配置文件

```sh
cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
```

### 12、加载br_netfilter模块

```sh
modprobe br_netfilter
```

### 13、更新配置文件

```sh
sysctl -p /etc/sysctl.d/k8s.conf
```

### 14、查看是否加载

```sh
lsmod | grep br_netfilter
```

### 15、使其生效

```sh
sysctl --system
```

### 升级Linux内核

### 1、导入elrepo gpg key

```sh
rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
```

### 2、安装elrepo yum源仓库

```sh
yum -y install https://www.elrepo.org/elrepo-release-7.0-4.el7.elrepo.noarch.rpm
```

### 3、安装kernel-ml版本,ml为长期稳定版本，lt为长期维护版本

```sh
yum --enablerepo="elrepo-kernel" -y install kernel-lt.x86_64
```

### 4、设置grub2默认引导为0

```sh
grub2-set-default 0
```

### 5、重新生成grub2引导文件

```sh
grub2-mkconfig -o /boot/grub2/grub.cfg
```

### 6、更新后重启系统

```sh
reboot
```

### 7、查看内核

```sh
uname -r
```

### 安装docker-ce和cri-dockerd

1、安装阿里云yum源

```sh
wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo
```

2、安装docker-ce

```sh
yum install docker-ce
```

3、启动docker服务

```sh
systemctl enable --now docker
```

4、yum repolist

5、修改cgroup方式

`在/etc/docker/daemon.json`

```sh
{
"exec-opts": ["native.cgroupdriver=systemd"]
}
```

6、下载cri-dockerd

```sh
wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.10/cri-dockerd-0.3.10-3.el7.x86_64.rpm
```

7、安装cri-dockerd

```sh
yum -y install cri-dockerd-0.3.10-3.el7.x86_64.rpm
```

8、修改/usr/lib/systemd/system/cri-docker.service 指定基本版本

```sh
vim /usr/lib/systemd/system/cri-docker.service
#修改第10行
 ExecStart=/usr/bin/cri-dockerd --pod-infra-container-image=registry.k8s.io/pause:3.9 --container-runtime-endpoint fd://

```

添加阿里云源

```sh
ExecStart=/usr/bin/cri-dockerd --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9 --container-runtime-endpoint fd://
```

9、启动cri-docker

```sh
systemctl start cri-docker
systemctl enable cri-docker
```

## 安装k8s

### 1、kubernetes yum源准备

```sh
cat > /etc/yum.repos.d/k8s.repo <<EOF
[kubernetes]
name=kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/repodata/repomd.xml.key
EOF
```

### 2、查看有没有要安装的软件

```sh
yum list | grep kubeadm
yum list | grep kubectl
yum list | grep kubelet
```

### 3、默认安装

```sh
yum -y install kubeadm kubectl kubelet
```

### 4、安装指定版本

```sh
yum -y install kubeadm-1.29.2-150500.1.1 kubectl-1.29.2-150500.1.1 kubelet-1.29.2-150500.1.1
```

### 5、配置kubelet

```sh
vim /etc/sysconfig/kubelet
#将下面的配置文件复制进去
KUBELET_EXTRA_ARGS="--cgroup-driver=systemd"
```

### 6、设置kubelet为开机自启动，由于没有生成配置文件，集群初始化后自动启动

```sh
systemctl enable kubelet
```

### 7、集群镜像准备

```sh
kubeadm config images list --kubernetes-version=v1.29.2
```

### 8、镜像下载，当出现两个容器运行时时，要在后面加--cri-socket unix:///var/run/cri-dockerd.sock 指定容器

```sh
kubeadm config images pull --cri-socket unix:///var/run/cri-dockerd.sock
```

使用阿里云源下载

```sh
kubeadm config images pull --cri-socket unix:///var/run/cri-dockerd.sock --image-repository registry.aliyuncs.com/google_containers
```

### 9、master节点集群初始化

```sh
kubeadm init --kubernetes-version=v1.29.2 --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.127.145 --cri-socket unix:///var/run/cri-dockerd.sock --image-repository registry.aliyuncs.com/google_containers
```

### 10、清除配置

```sh
#删除kubernetes文件夹下面的所有文件
rm -rf /etc/kubernetes/*
```

```sh
kubeadm reset --cri-socket unix:///var/run/cri-dockerd.sock
```

### 11、将工作节点连接主节点的秘钥

```sh
kubeadm join 192.168.127.145:6443 --token xy677h.6r4py7icfjgibuzh \
        --discovery-token-ca-cert-hash sha256:583a4449f279f6885478d8902b7d8f9274792fb906f4ce1649b3b379b5d1f9fb
```

### 12、node节点或其他控制平面节点加入集群时，

```sh
Found multiple CRI endpoints on the host. Please define which one do you wish to use by setting the 'criSocket' field in the kubeadm configuration file: unix:///var/run/containerd/containerd.sock, unix:///var/run/cri-dockerd.sock
To see the stack trace of this error execute with --v=5 or higher
```

报错解决办法：

```sh
kubeadm join 192.168.127.145:6443 --token xy677h.6r4py7icfjgibuzh \
        --discovery-token-ca-cert-hash sha256:583a4449f279f6885478d8902b7d8f9274792fb906f4ce1649b3b379b5d1f9fb \
 --cri-socket unix:///var/run/cri-dockerd.sock
```

### 13、设置操作权限

```sh
 mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

### 14、token 过期解决办法

```sh
#生成命令
kubeadm token create
#token秘钥
566lyl.up1ebpt9k0w57qbc
#添加到加入命令
kubeadm join 192.168.127.145:6443 --token 566lyl.up1ebpt9k0w57qbc \
--discovery-token-ca-cert-hash sha256:b6f52ab55434b44356ad4586436e586aba8a98e54125a179452bb8ae1f616527 --cri-socket unix:///var/run/cri-dockerd.sock
```

### 15、证书密钥的获取方式

```sh
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
```

### 16、部署网络插件calico

https://raw.githubusercontent.com/projectcalico/calico/v3.27.2/manifests/tigera-operator.yaml

复制代码到本地创建tigera-operator.yaml文件

### 16.1、(离线安装)下载网络插件容器包  https://github.com/projectcalico/calico/releases 下载[release-v3.27.2.tgz](https://github.com/projectcalico/calico/releases/download/v3.27.2/release-v3.27.2.tgz) 这个是专门给容器使用的

```sh
 tar -zxvf release-v3.27.2.tgz
 #解压完之后进入images目录
 /root/release-v3.27.2/images
 #里面有三个文件，将这个三个文件做成容器
 calico-cni.tar  calico-node.tar  calico-kube-controllers.tar
```

解压出来的文件导入容器里面（这个是docker的方法）

```sh
docker load -i calico-cni.tar
docker load -i calico-node.tar
docker load -i calico-kube-controllers.tar
```

containerd的方法，k8s.io是容器的命名空间

```sh
ctr -n k8s.io images import calico-cni.tar
ctr -n k8s.io images import calico-kube-controllers.tar
ctr -n k8s.io images import calico-node.tar
```

启动网络

```sh
kubectl create -f tigera-operator.yaml
```

```sh
#删除pod
kubectl delete -f tigera-operator.yaml
```

查看是否启动成功

```sh
kubectl get pod -n tigera-operator -o wide
```

### 17、下载kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.2/manifests/custom-resources.yaml

配置网络

```yaml
#vim custom-resources.yaml 修改成自己的网段
cidr: 10.244.0.0/16
```

启用网络配置(使用这个文件)

```sh
kubectl create -f custom-resources.yaml
```

查看网络pod

```sh
watch kubectl get pods -n calico-system
```

#### k8s使用及相关命令

1、查看集群的健康状况

```sh
kubectl get cs
```

2、查看k8sDNS的命令

```sh
kubectl get svc -n kube-system
```

3、安装dns工具

```sh
 sudo yum install bind-utils
```

4、检验DNS是否正常

```sh
dig -t a www.baidu.com @10.96.0.10
```

## pod基本操作

### 1、**查看所有的命名空间**

```sh
kubectl get pods|pod -A
```

### 2、**查看指定的命名空间**

```sh
kubectl get pods|pod |po -n 命名空间名称
```

### 3、**查看命名空间下pod的详细信息**

```sh
kubectl get pods -o wide -n 命名空间名称
```

### 4、**查看所有的命名空间下pod的详细信息**

```sh
kubectl get pods -o wide -A
```

### 5、**实时监控pod的状态**

```sh
kubectl get pod -w
#看所有pod的监控
kubectl get pod -A -w
```

### 6、**创建pod**

```sh
kubectl create -f <pod-definition.yaml>
```

### 6.1、**以 YAML文件创建pod**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx  # 设置 Pod 的名称为 "nginx"
spec:
  containers:
  - name: nginx  # 设置容器的名称为 "nginx"
    image: nginx:1.25  # 使用的容器镜像为 "nginx:1.14.2"
    ports:
    - containerPort: 80  # 容器中 Nginx 服务监听的端口为 80
```

### 7、**删除 Pod**

```sh
kubectl delete pod <pod-name>
kubectl delete -f yaml文件名称

1. 删除特定的 Pod
kubectl delete pod ingress-nginx-admission-create-t7v67 -n ingress-nginx
kubectl delete pod myingress-ingress-nginx-admission-create-hfwcg -n ingress-nginx
2. 删除相应的 Job
kubectl delete job ingress-nginx-admission-create -n ingress-nginx
kubectl delete job myingress-ingress-nginx-admission-create -n ingress-nginx
3. 删除相关的 Helm Release
helm uninstall myingress -n ingress-nginx
4. 检查资源
kubectl get pods -n ingress-nginx
kubectl get jobs -n ingress-nginx
```

### 8、**执行 YAML文件**

```sh
kubectl apply -f yaml文件名称
kubectl create -f yaml文件名称
#apply和create 区别，create不存在pod时才创建，存在就报错，apply存在就更新，不存在就创建，一般都是使用apply
```

### 9、**进入pod**

```sh
kubectl exec -it <pod-name> -- /bin/bash
#进入指定容器
kubectl exec -it pod名称 -c 容器名称 /bin/bash
#列如
kubectl exec -it nginx -c nginx -- bash
```

### 10、**查看pod日志**

```sh
#查看NGINX日志
kubectl logs -f nginx
#查看指定容器的日志
kubectl logs -f pod名称 -c 容器名称
```

### 11、**查看pod资源使用情况**

```sh
kubectl top pod 
```

### 12、**查看pod标签**

```sh
kubectl get nodes --show-labels
```

### 13、**选择一个节点，给它添加一个标签**

```sh
kubectl label nodes 节点名称 disktype=sdd #disktype=sdd 是键和值
```



## 容器的基本操作

### 1、容器生命周期和回调

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: lifecycle-demo
spec:
  containers:
  - name: lifecycle-demo-container
    image: nginx
    lifecycle:
      postStart:
        exec:
          command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
      preStop:
        exec:
          command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
```

### 安装 metrics-server-0.6.4 （安装不了也没有关系，可视化工具可以解决）

### 1、**下载地址：kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.6.4/components.yaml**

### 2、**修改成国内镜像**

```yaml
 containers:
      - args:
        - --cert-dir=/tmp
        - --secure-port=4443
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-use-node-status-port
        - --metric-resolution=15s
        image: registry.cn-hangzhou.aliyuncs.com/rainux/metrics-server:v0.6.4
        imagePullPolicy: IfNotPresent
#修改成下面这个镜像
registry.cn-hangzhou.aliyuncs.com/rainux/metrics-server:v0.6.4

docker tag registry.cn-hangzhou.aliyuncs.com/rainux/metrics-server:v0.6.4 registry.k8s.io/metrics-server/metrics-server:v0.6.4
```

### 3、**安装metrics-server**

```sh
kubectl apply -f components.yaml
```

### 4、**查看是否在运行**

```sh
kubectl get pods -n kube-system | grep metrics
```

### 5、**查看pod状态的变化**

```sh
watch -n 1 -d kubectl get pod -A #1秒变化
```

## deployment控制器

### 1、**deployment第一个示例**

```yaml
apiVersion: apps/v1 #这指定了所使用的 Kubernetes API 的版本，apps/v1 版本支持 Deployment 资源。
kind: Deployment #这定义了 Kubernetes 中的资源类型，即部署（Deployment）。
metadata: #这是 Deployment 的元数据，包括名称（name）和标签（labels）
  name: nginx-deployment #定义了部署的名称为 "nginx-deployment"
  labels: #标签用于标识 Deployment，这里标签为 "app: nginx"。
    app: nginx
spec: #这是 Deployment 的规格，指定了部署的配置。
  replicas: 3 #定义了要创建的 Pod 副本数，即部署中运行的实例数量为 3。
  selector: #选择器用于选择应该由该 Deployment 管理的 Pod。
    matchLabels: #指定了要匹配的标签，这里选择标签为 "app: nginx" 的 Pod。
      app: nginx 
  template: #定义了要创建的 Pod 的模板。
    metadata: # Pod 模板的元数据。
      labels: #定义了 Pod 的标签，这里标签为 "app: nginx"。
        app: nginx
    spec: #pod 的规格，包括容器等。
      containers: #定义了要在 Pod 中运行的容器列表
      - name: nginx #容器的名称为 "nginx"。
        image: nginx:1.14.2 #nginx:1.14.2`: 指定了要使用的 Docker 镜像，此处为 Nginx 的版本 1.14.2。
        ports: # 定义了容器的端口映射。
        - containerPort: 80 #指定了容器监听的端口号为 80，即 Nginx 服务器的默认 HTTP 端口
```

### 2、**查看deployment**

```sh
kubectl get deployment
```

### 3、**deployment相关命令**

```sh
#部署应用
kubectl apply -f app.yaml
#查看pod
kubectl get pod -o -wide
#查看pod详细信息
kubectl describe pod pod-name
#查看log
kubectl logs -f pod-name
#进入pod终端
kubectl exec -it pod-name --bash
#删除Deployment
kubectl delete deployment [DEPLOYMENT_NAME]
#编辑Deployment
kubectl edit deployment [DEPLOYMENT_NAME]
```

### 4、**扩缩deployment**

```sh
#查看副本
kubectl get rs|replicaset
#伸缩扩展副本
kubectl scale deployment nginx --replicas=5
```

### 5、**回滚deployment**

```sh
#查看上线状态
kubectl rollout status [deployment nginx-deployment | deployment/nginx]
#查看版本历史
kubectl rollout history deployment nginx-deployment
#查看某次历史的详细信息
kubectl rollout history deployment/nginx-deployment --revision=2
#回到上个版本
kubectl rollout undo deployment nginx-deployment
#回滚到指定版本
kubectl rollout undo deployment nginx-deployment --to-revision=2
#重新部署
kubectl rollout restart deployment nginx-deployment
#暂停运行，暂停后，对deployment的修改不会立即生效，恢复后才利用设置
kubectl rollout pause deployment nginx-deployment
#恢复
kubectl rollout resume deployment nginx-deployment
```

### 6、**删除deployment**

```sh
#删除deployment
kubectl delete deployment nginx-deployment
kubectl delete -f nginx-deployment.yaml (强烈建议使用该方法)
#删除全部资源
kubectl delete all --all
#删除指定命名空间的名称
kubectl delete all --all -n 命名空间的名称
```

### 7、下面是一个 Deployment 示例。其中创建了一个 ReplicaSet，负责启动三个 `nginx` Pod

```yaml
apiVersion: apps/v1  # 使用的 Kubernetes API 版本
kind: Deployment  # 定义的 Kubernetes 资源类型为 Deployment
metadata:  # 元数据段开始
  name: nginx-deployment  # Deployment 的名称为 "nginx-deployment"
  labels:  # 标签段开始
    app: nginx  # Deployment 的标签为 "app=nginx"
spec:  # 规范段开始
  replicas: 3  # 需要维护的 Pod 副本数为 3
  selector:  # 选择器段开始
    matchLabels:  # 匹配标签段开始
      app: nginx  # 选择标签为 "app=nginx" 的 Pod
  template:  # Pod 模板段开始
    metadata:  # 元数据段开始
      labels:  # 标签段开始
        app: nginx  # Pod 模板的标签为 "app=nginx"
    spec:  # 规范段开始
      containers:  # 容器列表
      - name: nginx  # 容器名称为 "nginx"
        image: nginx:1.14.2  # 使用的容器镜像为 nginx:1.14.2
        ports:  # 容器端口列表
        - containerPort: 80  # 容器内部监听的端口为 80
```

## 用docker安装Kuboard(可视化工具)

### 1、**创建docker-compose.yml**

```yaml
version: '3.8'
services:
  kuboard:
    image: eipwork/kuboard:v3
    container_name: kuboard
    restart: unless-stopped
    ports:
      - "80:80"
      - "10081:10081"
    environment:
      - KUBOARD_ENDPOINT=http://192.168.128.136:80
      - KUBOARD_AGENT_SERVER_TCP_PORT=10081
    volumes:
      - /root/kuboard-data:/data
```

## statefulSet控制器

### **1、搭建NFS服务，创建新的系统安装**

```sh
#安装nfs-utils
yum install -y rpcbind nfs-utils
#创建一个数据目录
mkdir -p /root/nfs/data
#编辑/etc/exports输入如下内容
#insecure：通过1024以上端口发送 rw：读写 sync：请求时写入共享 no_root_squash:root用户有完全根目录访问权限
echo "/root/nfs/data*(insecure,rw,sync,no_root_squash)" >> /etc/exports

#启动相关服务并配置开机自启动
systemctl start rpcbind
systemctl start nfs-server
systemctl enable rpcbind
systemctl enable nfs-server
#重新挂载，使 /etc/exports生效
exports -r
#查看共享情况
exportfs
```

### **2、安装客户端（在K8S所有的节点安装）**

```sh
#在所有节点上安装客户端
yum install -y nfs-utils
#创建本地目录
mkdir -p /root/nfs
#挂载远程NFS目录到本地 ip 是nfs
mount -t nfs nfs服务器地址IP :/root/nfs /root/nfs
#写入一个测试文件
echo "hello nfs server" > /root/nfsmount/test.txt
#去远程nfs目录查看
cat /root/nfsmount/test.txt
#挂载取消
umount -f -l nfs目录
```

### **3、下面的示例演示了 StatefulSet 的组件**

```yaml
apiVersion: v1  # 使用的 Kubernetes API 版本
kind: Service  # 定义的 Kubernetes 资源类型为 Service
metadata:  # 元数据段开始
  name: nginx  # 服务的名称为 "nginx"
  labels:  # 标签段开始
    app: nginx  # 服务的标签为 "app=nginx"
spec:  # 规范段开始
  ports:  # 端口列表
  - port: 80  # 服务监听的端口为 80
    name: web  # 端口的名称为 "web"
  clusterIP: None  # 该服务没有集群内部 IP 地址，通常用于 headless 服务
  selector:  # 选择器段开始
    app: nginx  # 选择标签为 "app=nginx" 的 Pod 作为后端
---
apiVersion: apps/v1  # 使用的 Kubernetes API 版本
kind: StatefulSet  # 定义的 Kubernetes 资源类型为 StatefulSet
metadata:  # 元数据段开始
  name: web  # StatefulSet 的名称为 "web"
spec:  # 规范段开始
  selector:  # 选择器段开始
    matchLabels:  # 匹配标签段开始
      app: nginx  # 选择标签为 "app=nginx" 的 Pod
  serviceName: "nginx"  # 该 StatefulSet 使用的服务名称为 "nginx"
  replicas: 3  # 需要维护的 Pod 副本数为 3
  minReadySeconds: 10  # Pod 最小就绪时间为 10 秒
  template:  # Pod 模板段开始
    metadata:  # 元数据段开始
      labels:  # 标签段开始
        app: nginx  # Pod 模板的标签为 "app=nginx"
    spec:  # 规范段开始
      terminationGracePeriodSeconds: 10  # Pod 终止的优雅期为 10 秒
      containers:  # 容器列表
      - name: nginx  # 容器名称为 "nginx"
        image: registry.k8s.io/nginx-slim:0.8  # 使用的容器镜像
        ports:  # 容器端口列表
        - containerPort: 80  # 容器内部监听的端口为 80
          name: web  # 暴露的端口名字为 "web"
        volumeMounts:  # 挂载的卷列表
        - name: www  # 挂载的卷名称为 "www"
          mountPath: /usr/share/nginx/html  # 挂载到容器内的路径
  volumeClaimTemplates:  # 动态 PVC 模板列表
  - metadata:  # 元数据段开始
      name: www  # PVC 的名称为 "www"
    spec:  # 规范段开始
      accessModes: [ "ReadWriteOnce" ]  # 读写权限为一次性
      storageClassName: "my-storage-class"  # 存储类名称为 "my-storage-class"
      resources:  # 资源声明段开始
        requests:  # 请求声明段开始
          storage: 1Gi  # 请求的存储资源为 1Gi

```

### 4、安装nfs-subdir-external-provisioner

calss.yaml

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-client
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
parameters:
  archiveOnDelete: "false"
```

deployment

```yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: nfs-client-provisioner
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nfs-client-provisioner
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: <YOUR NFS SERVER HOSTNAME>
            - name: NFS_PATH
              value: /var/nfs
      volumes:
        - name: nfs-client-root
          nfs:
            server: <YOUR NFS SERVER HOSTNAME>
            path: /var/nfs
```

rbac 下面的 ClusterRole 和 RoleBinding 将允许用户 `user-1` 把名字空间 `user-1-namespace` 中的 `admin`、`edit` 和 `view` 角色赋予其他用户：

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: role-grantor
rules:
- apiGroups: ["rbac.authorization.k8s.io"]
  resources: ["rolebindings"]
  verbs: ["create"]
- apiGroups: ["rbac.authorization.k8s.io"]
  resources: ["clusterroles"]
  verbs: ["bind"]
  # 忽略 resourceNames 意味着允许绑定任何 ClusterRole
  resourceNames: ["admin","edit","view"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: role-grantor-binding
  namespace: user-1-namespace
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: role-grantor
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: user-1
```

## daemonset 控制器

### 1、创建一个实例

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: kube-system
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      # 这些容忍度设置是为了让该守护进程集在控制平面节点上运行
      # 如果你不希望自己的控制平面节点运行 Pod，可以删除它们
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
      # 可能需要设置较高的优先级类以确保 DaemonSet Pod 可以抢占正在运行的 Pod
      # priorityClassName: important
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
```

### 2、基于 YAML 文件创建 DaemonSet：

```yaml
kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml
```

### 3、Job

```yaml
```

## helm方式部署rancher

### helm 安装

### 1、官网:[https://github.com/helm/helm/releases](https://github.com/helm/helm/releases)

### 2、解压(`tar -zxvf helm-v3.0.0-linux-amd64.tar.gz`)

### 3、在解压目录中找到`helm`程序，移动到需要的目录中(`mv linux-amd64/helm /usr/local/bin/helm`)

### 部署rancher

### 1、修改代理模式

```sh
kubectl edit configmap kube-proxy -n kube-system
```



### 2、修改代码：

```yaml
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  config.conf: |-
    apiVersion: kubeproxy.config.k8s.io/v1alpha1
    bindAddress: 0.0.0.0
    bindAddressHardFail: false
    clientConnection:
      acceptContentTypes: ""
      burst: 0
      contentType: ""
      kubeconfig: /var/lib/kube-proxy/kubeconfig.conf
      qps: 0
    clusterCIDR: 10.244.0.0/16
    configSyncPeriod: 0s
    conntrack:
      maxPerCore: null
      min: null
      tcpBeLiberal: false
      tcpCloseWaitTimeout: null
      tcpEstablishedTimeout: null
      udpStreamTimeout: 0s
      udpTimeout: 0s
    detectLocal:
      bridgeInterface: ""
      interfaceNamePrefix: ""
    detectLocalMode: ""
    enableProfiling: false
    healthzBindAddress: ""
    hostnameOverride: ""
    iptables:
      localhostNodePorts: null
      masqueradeAll: false
      masqueradeBit: null
      minSyncPeriod: 0s
      syncPeriod: 0s
    ipvs:
      excludeCIDRs: null
      minSyncPeriod: 0s
      scheduler: ""
      strictARP: false #将false改为true
      syncPeriod: 0s
      tcpFinTimeout: 0s
      tcpTimeout: 0s
      udpTimeout: 0s
 kind: KubeProxyConfiguration
    logging:
      flushFrequency: 0
      options:
        json:
          infoBufferSize: "0"
      verbosity: 0
    metricsBindAddress: ""
    mode: "" #设置ipvs
    nftables:
      masqueradeAll: false
      masqueradeBit: null
      minSyncPeriod: 0s
      syncPeriod: 0s
    nodePortAddresses: null
    oomScoreAdj: null
    portRange: ""
    showHiddenMetricsForVersion: ""
    winkernel:
      enableDSR: false
      forwardHealthCheckVip: false
      networkName: ""
      rootHnsEndpointName: ""
      sourceVip: ""
  kubeconfig.conf: |-
    apiVersion: v1
    kind: Config
    clusters:
    - cluster:
        certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        server: https://192.168.127.145:6443
      name: default
    contexts:
    - context:
        cluster: default
        namespace: default
        user: default
      name: default
    current-context: default
    users:
    - name: default
      user:
        tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
kind: ConfigMap
metadata:
  annotations:
    kubeadm.kubernetes.io/component-config.hash: sha256:37963ff79d2fa26b47fe50ea27dafc33a89b03bb47eb574f3606c776a95568ba
  creationTimestamp: "2024-03-12T02:46:42Z"
  labels:
    app: kube-proxy
  name: kube-proxy
  namespace: kube-system
  resourceVersion: "229"
  uid: e61637db-2973-4401-afc2-a1312d95af6e

```

### 3、重启代理

```sh
#查看应用
kubectl get daemonset -n kube-system
#重启
kubectl rollout restart daemonset kube-proxy -n kube-system
#查看pod日志
 kubectl describe pod kube-proxy-htspv -n kube-system
```

### 4、安装MetalLB

- 安装文档：https://metallb.universe.tf/installation/ 如果网络不能正常访问github，就用这个地址下载：https://raw.githubusercontent.com/metallb/metallb/v0.14.5/config/manifests/metallb-native.yaml 在上传到服务器

- 安装：

  ```sh
  shkubectl apply -f metallb-native.yaml
  ```

- 添加IP地址池：

  ```yaml
  apiVersion: metallb.io/v1beta1
  kind: IPAddressPool
  metadata:
    name: first-pool
    namespace: metallb-system
  spec:
    addresses:
    - 192.168.1.240-192.168.1.250
  ```

  创建地址池：

  ```ssh
  kubectl apply -f ippool.yaml
  ```

  查看IP地址池：

  ```sh
  kubectl get ipaddresspool -n metallb-system
  ```

  开启二层宣告

  ```yaml
  apiVersion: metallb.io/v1beta1
  kind: L2Advertisement
  metadata:
    name: example
    namespace: metallb-system
  ```

  创建二层宣告：

  ```yaml
  kubectl apply -f l2.yaml
  ```


### 5、安装ingress-nginx代理服务器

官网地址：https://github.com/kubernetes/ingress-nginx/blob/main/docs/deploy/index.md

1、添加阿里云仓库

```sh
helm repo add aliyuncs https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
helm repo update
```

2、搜索仓库

```sh
helm search repo aliyuncs
```

```sh
[root@master ingress-nginx]# sed -i '/registry:/s#registry.k8s.io#registry.cn-hangzhou.aliyuncs.com#g' values.yaml
[root@master ingress-nginx]# sed -i 's#ingress-nginx/controller#yinzhengjie-k8s/ingress-nginx#' values.yaml
[root@master ingress-nginx]# sed -i 's#ingress-nginx/kube-webhook-certgen#yinzhengjie-k8s/ingress-nginx#' values.yaml
[root@master ingress-nginx]# sed -i 's#v1.10.1#kube-webhook-certgen-v1.10.1#' values.yaml
[root@master ingress-nginx]# sed -ri '/digest:/s@^@#@' values.yaml
[root@master ingress-nginx]# sed -i '/hostNetwork:/s#false#true#' values.yaml
[root@master ingress-nginx]# sed -i  '/dnsPolicy/s#ClusterFirst#ClusterFirstWithHostNet#' values.yaml
[root@master ingress-nginx]# sed -i '/kind/s#Deployment#DaemonSet#' values.yaml
[root@master ingress-nginx]# sed -i '/default:/s#false#true#'  values.yaml

```

