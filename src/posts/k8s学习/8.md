
---
icon: pen-to-square
date: 2026-1-28
category:
  - Linux
tag:
  - docker
  - Kubernetes
  - HAProxy
  - Keepalived
star: true
sticky: true #标记
---

#  Sealos 安装 HA Kubernetes 集群

> 官网:https://sealos.run/docs/k8s/quick-start/deploy-kubernetes

## 框架

```lua
        ┌───────────────────────┐                                    
        │        VIP             │
        │ 192.168.1.238:6443   │
        └─────────▲─────────────┘
                  │
        ┌─────────┴─────────┐
        │                   │
┌──────────────┐     ┌──────────────┐                          
│   LB01       │     │   LB02       │
│ Keepalived   │     │ Keepalived   │
│ HAProxy      │     │ HAProxy      │
└───────▲──────┘     └───────▲──────┘
        │                     │
        └──────┬──────────────┘
               │ 6443
┌──────────────┼──────────────────────────┐
│              │                          │
│   master01   │   master02   │  master03 │
│   CP+etcd    │   CP+etcd    │  CP+etcd  │
│              │                          │
└──────────────┼──────────────────────────┘
               │
       ┌───────┴────────┐
       │                │
  worker01           worker02

```

## 准备工作

### 修改主机名字

```sh
sudo hostnamectl set-hostname 新主机名
```

### 关闭防火墙（每台机器）

```sh
sudo ufw disable
sudo systemctl is-enabled ufw
```

### 关闭 Swap

```sh
sudo sed -i '/swap/ s/^\(.*\)$/#\1/g' /etc/fstab
```

### 设置桥接转发

```sh
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables=1
net.ipv4.ip_forward=1
EOF

sudo sysctl --system
```

### 时间同步（必须）

```sh
sudo apt install -y chrony
sudo timedatectl set-timezone Asia/Shanghai
sudo systemctl restart chronyd
```

### 关闭 AppArmor（Ubuntu 必须，否则容器可能异常）

```sh
sudo systemctl stop apparmor
sudo systemctl disable apparmor
```

### 配置 hosts（所有节点一致）

```sh
sudo vim /etc/hosts
# 填写
192.168.1.230 master01
192.168.1.231 master02
192.168.1.232 master03
192.168.1.234 worker01
192.168.1.235 worker02
```

## 准备IP

| 节点名称 | IP              | 用户名称 | 密码           | 备注       |
| -------- | --------------- | -------- | -------------- | ---------- |
| master01 | 192.168.1.230 | mes      | 123456 | 主节点     |
| master02 | 192.168.1.231 | mes      | 123456 | 主节点     |
| master03 | 192.168.1.232 | mes      | 123456 | 主节点     |
| worker01 | 192.168.1.234 | mes      | 123456 | 工作节点   |
| worker02 | 192.168.1.235 | mes      | 123456 | 工作节点   |
| nfs      | 192.168.1.233 | mes      | 123456 | 文件服务器 |
| lb01     | 192.168.1.236 | lb01     | 123456 | vip        |
| lb02     | 192.168.1.237 | lb01     | 123456 | vip        |

## 部署 HAProxy + Keepalived（实现 VIP）（非常重要）

`一定要去了解原理，不然无法理解这套高可用怎么运作`

> Keepalived：https://keepalived.readthedocs.io/en/latest/index.html
> HAProxy：https://www.haproxy.com/documentation/haproxy-configuration-tutorials/

### Keepalived + HAProxy 是什么？

它们是**实现 Kubernetes 高可用的两套组件**：

- **HAProxy**
   → 一个高性能负载均衡器
   → 把流量分发给多个 LB01的 API Server
- **Keepalived**
   → 用来实现 **VIP 漂移**（虚拟 IP 浮动）
   → 保证当某个 LB01挂了，VIP 自动漂移到LB02

两者配合，就能保证：

API Server 有统一入口(VIP)
某个 LB01挂了，另一个 LB02自动接管
整个 k8s 控制面保持可访问

### **Keepalived 的作用**：

创建一个可漂移的虚拟 IP（VIP）

当 LB01活着：

```nginx
VIP → LB01
```

LB01 挂了：

```nginx
VIP 自动漂移 → LB02
```

LB02挂了：

```nginx
VIP 自动漂移 → LB01
```

客户端永远访问 VIP，无需判断 master 状态

### **HAProxy 的作用：**

把 VIP 访问转发给真正健康的 master：

```makefile
VIP:6443 → HAProxy → 所有健康的 master
```

它会自动探测各 master API Server 是否存活。

### 安装HAProxy + Keepalived

### 规划

| 角色 | IP                  | 描述                                |
| ---- | ------------------- | ----------------------------------- |
| lb01 | 192.168.1.236     | 运行 HAProxy + Keepalived（MASTER） |
| lb02 | 192.168.1.237     | 运行 HAProxy + Keepalived（BACKUP） |
| VIP  | **192.168.1.238** | 虚拟 IP（对外统一访问）             |

**VIP 不需要在任何机器上配置，只是 Keepalived 自动漂移的一个虚拟 IP。**

### 安装 HAProxy（两台 LB01\LB02都要执行）

```sh
sudo apt update
sudo apt install -y haproxy keepalived ipvsadm
```

### 配置 HAProxy（两台 LB01\LB02 填一样）

#### 编辑配置文件

```sh
sudo vim /etc/haproxy/haproxy.cfg
```

#### 配置 HAProxy（两台一样）

```cfg
global
    log /dev/log local0
    log /dev/log local1 notice
    daemon
    maxconn 200000

defaults
    log     global
    mode    tcp
    option  tcplog
    timeout connect 5s
    timeout client  1m
    timeout server  1m

frontend k8s-apiserver
    bind 192.168.1.238:6443
    default_backend k8s-apiserver

backend k8s-apiserver
    balance roundrobin
    option tcp-check
    default-server inter 3s fall 3 rise 2

    server master01 192.168.1.230:6443 check
    server master02 192.168.1.231:6443 check
    server master03 192.168.1.232:6443 check

# ================
# HAProxy Web UI,生产环境不要开启这个功能，有安全风险
# ================
listen stats
    bind 192.168.1.238:9000
    mode http
    stats enable
    stats uri /stats
    stats refresh 5s
    stats auth admin:123456

```

#### 配置web UI

```cfg
# ================
# HAProxy Web UI
# ================
listen stats
    bind 192.168.1.238:9000
    mode http
    stats enable
    stats uri /stats
    stats refresh 5s
    stats auth admin:123456
```

### 保存并重启

```sh
sudo systemctl enable haproxy
sudo systemctl restart haproxy
sudo systemctl status haproxy
```

### 配置 Keepalived

#### LB01（MASTER）配置

```sh
sudo vim /etc/keepalived/keepalived.conf
```

```c
global_defs {
   router_id LVS_K8S
}

vrrp_script chk_haproxy {
    script "pidof haproxy"
    interval 2
    weight -20
}

vrrp_instance VI_1 {
    state MASTER
    interface ens3
    virtual_router_id 51
    priority 120
    advert_int 1

    authentication {
        auth_type PASS
        auth_pass k8s-ha
    }

    virtual_ipaddress {
        192.168.1.238/24
    }

    track_script {
        chk_haproxy
    }
}
```

#### LB02（BACKUP）配置

```sh
sudo vim /etc/keepalived/keepalived.conf
```

```c
global_defs {
   router_id LVS_K8S
}

vrrp_script chk_haproxy {
    script "pidof haproxy"
    interval 2
    weight -20
}

vrrp_instance VI_1 {
    state BACKUP
    interface ens3
    virtual_router_id 51
    priority 100
    advert_int 1

    authentication {
        auth_type PASS
        auth_pass k8s-ha
    }

    virtual_ipaddress {
        192.168.1.238/24
    }

    track_script {
        chk_haproxy
    }
}
```

#### 启用

```sh
sudo systemctl restart keepalived
sudo systemctl enable keepalived
sudo systemctl status keepalived
```

#### 成功

```sh
lb01@lb01:/etc/keepalived$ ip a | grep 192.168.1.238
    inet 192.168.1.238/24 scope global secondary ens3
lb01@lb01:/etc/keepalived$ nc -zv 192.168.1.238 6443
Connection to 192.168.1.238 6443 port [tcp/*] succeeded!
```

#### 测试

```sh
sudo systemctl stop keepalived #停掉 MASTER 节点上的 keepalived，看是否漂移到BACKUP节点
ip addr show | grep 192.168.1.238 #查看是否还有输出，输出为空正常
#BACKUP节点
ip addr show | grep 192.168.1.238 #有输出正常

#测试完之后一定要做
sudo systemctl start keepalived #在MASTER 节点运行，确认 VIP 回到优先级高的节点（如果启用了抢占）表示高可用已成功部署
```

## 安装K8S（重要）

> 主节点建议为奇数个（如 3 或 5 个），而工作节点的数量可以是任何数目，偶数或奇数都可以，但要根据实际负载和资源情况来进行调整

### 安装Sealos

```sh
$ echo "deb [trusted=yes] https://apt.fury.io/labring/ /" | sudo tee /etc/apt/sources.list.d/labring.list
$ sudo apt update
$ sudo apt install sealos
```

### 安装集群

```sh
sudo sealos run \
  registry.cn-shanghai.aliyuncs.com/labring/kubernetes:v1.31.11 \
  registry.cn-shanghai.aliyuncs.com/labring/helm:v3.18.4 \
  registry.cn-shanghai.aliyuncs.com/labring/cilium:v1.14.19 \
  --masters 192.168.1.230,192.168.1.231,192.168.1.232 \
  --nodes 192.168.1.234,192.168.1.235 \
  -u mes \
  -p 123456
```

> 安装报错：2025-12-03T22:54:38 error unable to setup: executable file 'newuidmap' not found in $PATH, install package 'uidmap' first or consider running in root mode
>
> 2025-12-03T22:57:37 error unable to setup: executable file 'fuse-overlayfs' not found in $PATH, install package 'fuse-overlayfs' first or consider running in root mode
>
> 需要安装：
>
> sudo apt install -y uidmap
>
> 检查安装是否成
>
> which newuidmap
>
> which newgidmap
>
> sudo apt update
>
> sudo apt install -y fuse-overlayfs

### 删除集群

```sh
sudo sealos reset #卸载集群
sudo rm -rf /etc/kubernetes/ ~/.kube /var/lib/etcd/*
```

### 增加 K8s 节点

增加 node 节点：(要和其他工作节点一样的配置)

```sh
sealos add --nodes 192.168.64.21,192.168.64.19 
```

增加 master 节点(要和其他主节点一样的配置)

```sh
sealos add --masters 192.168.64.21,192.168.64.19 
```



### 配置证书**(非常重要，配置不好集群无法调度)**

#### 配置 kube-apiserver 使用 VIP 访问 etcd

/etc/kubernetes/manifests/kube-apiserver.yaml

```yaml
- --etcd-servers=https://127.0.0.1:2379
- --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
- --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
- --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
```

#### 解决权限问题

```sh
mkdir -p ~/.kube 
sudo cp /etc/kubernetes/admin.conf ~/.kube/config 
sudo chown $USER:$USER ~/.kube/config
```

#### 工作节点权限

```sh
#主节点复制过去
sudo scp /etc/kubernetes/admin.conf mes@worker02:/home/mes/
#工作节点创建文件夹
sudo mkdir -p /home/mes/.kube
sudo mv /home/mes/admin.conf /home/mes/.kube/config
sudo chown -R mes:mes /home/mes/.kube
chmod 600 /home/mes/.kube/config
```

#### 开启kubelet

```sh
systemctl restart kubelet
#查看是否运行
ps aux | grep kube-apiserver
```

#### 给 kube-apiserver 生成包含 VIP 的证书

```sh
kubeadm init phase certs apiserver \
    --apiserver-cert-extra-sans=192.168.1.236
```

#### 重新生成 kube-apiserver 证书：

```sh
sudo kubeadm init phase certs apiserver --apiserver-cert-extra-sans=192.168.1.238
```

#### 备份现有证书

```sh
sudo cp -r /etc/kubernetes/pki /etc/kubernetes/pki.bak
```

#### **删除旧的 apiserver 证书**：

```sh
sudo rm /etc/kubernetes/pki/apiserver.crt /etc/kubernetes/pki/apiserver.key
```

#### apiserver 使用的 etcd client 证书错误 / 过期 / 换过但未同步

##### 删除 apiserver 的 etcd client 证书（强制重建）

```sh
sudo rm -f /etc/kubernetes/pki/apiserver-etcd-client.crt
sudo rm -f /etc/kubernetes/pki/apiserver-etcd-client.key
```

##### 重签 etcd client 证书（使用 etcd CA）

```sh
#两个都要执行
sudo kubeadm init phase certs apiserver-etcd-client
sudo kubeadm init phase certs apiserver-kubelet-client
```

##### 重建所有 kubeconfig（确保 kube-apiserver 指向正确证书链）

```sh
sudo kubeadm init phase kubeconfig all
```

##### 重启 kubelet（强制静态 pod 重建）

```sh
sudo systemctl restart kubelet
#查看pod
sudo crictl ps | grep kube-apiserver
```

##### 备份旧证书

```sh
sudo cp -r /etc/kubernetes/pki /etc/kubernetes/pki.bak
```

##### 强制生成证书

```sh
sudo rm -f /etc/kubernetes/pki/apiserver.crt /etc/kubernetes/pki/apiserver.key
sudo rm -f /etc/kubernetes/pki/apiserver-kubelet-client.crt /etc/kubernetes/pki/apiserver-kubelet-client.key
sudo rm -f /etc/kubernetes/pki/apiserver-etcd-client.crt /etc/kubernetes/pki/apiserver-etcd-client.key

#生成证书
sudo kubeadm init phase certs apiserver \
  --apiserver-cert-extra-sans=192.168.1.238,master01,192.168.1.230,master02,192.168.1.231,master03,192.168.1.232,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster.local,apiserver.cluster.local

```

##### 重新生成证书

```sh
sudo kubeadm init phase certs apiserver-kubelet-client
```

##### 将证书同步到其他主节点

```sh
sudo scp /etc/kubernetes/pki/apiserver.crt mes@master02:/tmp/
sudo scp /etc/kubernetes/pki/apiserver.key mes@master02:/tmp/

sudo scp /etc/kubernetes/pki/apiserver.crt mes@master03:/tmp/
sudo scp /etc/kubernetes/pki/apiserver.key mes@master03:/tmp/

#覆盖
sudo mv /tmp/apiserver.crt /etc/kubernetes/pki/
sudo mv /tmp/apiserver.key /etc/kubernetes/pki/

#给权限
sudo chown root:root /etc/kubernetes/pki/apiserver.*
sudo chmod 600 /etc/kubernetes/pki/apiserver.key
sudo chmod 644 /etc/kubernetes/pki/apiserver.crt

#工作节点
sudo scp /etc/kubernetes/pki/ca.crt mes@worker01:/tmp/
sudo scp /etc/kubernetes/pki/ca.crt mes@worker02:/tmp/
#覆盖掉之前的文件
sudo mv /tmp/ca.crt /var/lib/kubelet/pki/ca.crt
```

#### 查看证书

```sh
sudo openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -text 
```

#### 查看日志

```sh
kubectl -n kube-system logs kube-scheduler-master01
```

#### 主节点配置文件(三个节点都要一样的配置)

admin.conf

```yaml
 server: https://192.168.1.238:6443 #修改成VIP
  name: kubernetes
```

 kubelet.conf

```yaml
server: https://192.168.1.238:6443 #https://apiserver.cluster.local:6443
  name: kubernetes
```

kube-apiserver.yaml

```yaml
 containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.168.1.230
    - --bind-address=0.0.0.0 #需要添加这个配置，其他的配置不用动
    - --allow-privileged=true
    - --audit-log-format=json
    - --audit-log-maxage=7
    - --audit-log-maxbackup=10
    - --audit-log-maxsize=100
    - --audit-log-path=/var/log/kubernetes/audit.log
    - --audit-policy-file=/etc/kubernetes/audit-policy.yml
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-aggregator-routing=true
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379 #这个地方不要走vip
    - --feature-gates=
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
```

##### 删掉`--controllers` 参数(所有节点都要删)

```sh
sudo vim /etc/kubernetes/manifests/kube-controller-manager.yaml
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-controller-manager
    tier: control-plane
  name: kube-controller-manager
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-controller-manager
    - --allocate-node-cidrs=true
    - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --bind-address=0.0.0.0
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --cluster-cidr=100.64.0.0/10
    - --cluster-name=kubernetes
    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
    - --cluster-signing-duration=876000h
    - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
    - --controllers=*,bootstrapsigner,tokencleaner #删掉这个一行，使用默认的controllers
    - --feature-gates=
    - --kubeconfig=/etc/kubernetes/controller-manager.conf
    - --leader-elect=true
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --root-ca-file=/etc/kubernetes/pki/ca.crt
    - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/22
    - --use-service-account-credentials=true
    image: registry.k8s.io/kube-controller-manager:v1.31.11
    imagePullPolicy: IfNotPresent
    livenessProbe:
```

controller-manager.conf

sudo vim /etc/kubernetes/controller-manager.conf

```yaml
  server: https://192.168.1.238:6443 #默认是这个apiserver.cluster.local 所有主节点都要修改
  name: kubernetes
```

 scheduler.conf  super-admin.conf

```yaml
  server: https://192.168.1.238:6443 #默认是这个 https://apiserver.cluster.local:6443所有主节点都要修改
  name: kubernetes
```

##### 工作节点修改访问地址

kubelet.conf

```yaml
  server: https://192.168.1.238:6443 #默认是这个 https://apiserver.cluster.local:6443所有主节点都要修改
  name: kubernetes
```

##### 运行流程

```text
kubelet
  ↓
VIP
  ↓
HAProxy
  ↓
任意 kube-apiserver
  ↓
etcd

kubectl / kubelet / controller
                    │
                    ▼
          ┌─────────────────────┐
          │   VIP 192.168.1.238│  ← Keepalived 漂移
          └─────────┬───────────┘
                    │
               HAProxy :6443
          ┌─────────┼───────────┐
          │          │           │
   master01      master02     master03
 kube-apiserver kube-apiserver kube-apiserver
          │          │           │
          └─────────┴───────────┘
                    │
               etcd 集群 (3)
```

## 配置containerd私有镜像仓库

#私有仓库1
sudo vim /etc/containerd/certs.d/sealos.hub:5000/hosts.toml

```yaml
server = "http://sealos.hub:5000"

[host."http://sealos.hub:5000"]
  capabilities = ["pull", "resolve", "push"]
  skip_verify = true

```

私有仓库2

sudo vim /etc/containerd/certs.d/192.168.1.166:80/hosts.toml

```yaml
server = "http://192.168.1.166:80"

[host."http://192.168.1.166:80"]
  capabilities = ["pull", "resolve", "push"]
  skip_verify = true
  username = "admin"
  password = "123456"
```

> 一个私有仓库一个文件，多个私有仓库不能同时写在一个文件里面

重启

```sh
sudo systemctl restart containerd
```

测试

```sh
sudo ctr -n k8s.io images pull \
  --plain-http \
  192.168.1.166:80/xxljob/xxl-job-admin:2.4.2
```

## 安装nfs

### 在一台服务器上安装nfs(独立一台服务器，不要装在集群里面)

```sh
sudo apt update
sudo apt install -y nfs-kernel-server
```

### 创建共享目录

```sh
sudo mkdir -p /data/nfs/k8s
sudo chown -R nobody:nogroup /data/nfs
sudo chmod -R 777 /data/nfs
```

### 配置导出目录

```sh
sudo vim /etc/exports
```

写入（示例网段）

```c
/data/nfs/k8s 192.168.1.0/24(rw,sync,no_subtree_check,no_root_squash)
```

生效：

```sh
sudo exportfs -arv
sudo systemctl enable nfs-server --now
```

在任意节点：

```sh
showmount -e 192.168.1.200   # 换成你的 NFS IP
```

### 第二步：K8s 所有节点安装 NFS 客户端

Ubuntu / Debian

```sh
sudo apt install -y nfs-common
```

CentOS / Rocky

```sh
sudo yum install -y nfs-utils
```

### 第三步：部署 NFS 动态存储**（核心）**

我们用 **官方维护的 provisioner**：

> nfs-subdir-external-provisioner

创建 namespace

```
kubectl create ns nfs-storage
```

创建 RBAC（必须）

```yaml
# ================================
# nfs-rbac.yaml
# 作用：
# 为 NFS 动态存储（nfs-subdir-external-provisioner）
# 提供创建 / 删除 PV、监听 PVC 的权限
# ================================

# -------------------------------
# ServiceAccount
# -------------------------------
# provisioner Pod 运行时使用的身份
# 后面所有权限都是“赋给这个账号的”
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner          # SA 名称（与 Deployment 中一致）
  namespace: nfs-storage                # 所在命名空间
---
# -------------------------------
# ClusterRole
# -------------------------------
# 集群级角色（因为 PV 是集群级资源）
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner

rules:
  # -------- PV 权限 --------
  # provisioner 需要：
  # - 创建 PV
  # - 删除 PV
  # - 监听 PV 变化
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]

  # -------- PVC 权限 --------
  # provisioner 需要：
  # - 监听 PVC 创建
  # - 在 PVC 绑定后更新状态
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]

  # -------- StorageClass 权限 --------
  # provisioner 需要读取 SC
  # 判断：
  # - provisioner 名称是否匹配
  # - 参数（archiveOnDelete 等）
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]

  # -------- Event 权限 --------
  # 用于在 kubectl describe pvc/pod 时
  # 能看到 “Provisioning succeeded / failed”
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
# -------------------------------
# lusterRoleBinding
# -------------------------------
# 把上面的 ClusterRole
# 绑定给 nfs-client-provisioner 这个 SA
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner

subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner      # 绑定的 SA
    namespace: nfs-storage             # SA 所在命名空间

roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner # 绑定的角色
  apiGroup: rbac.authorization.k8s.io

```

```sh
kubectl apply -f nfs-rbac.yaml
```

部署 Provisioner

```yaml
# nfs-provisioner.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  namespace: nfs-storage
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: 192.168.1.233
            - name: NFS_PATH
              value: /data/nfs
          volumeMounts:
            - name: nfs-volume
              mountPath: /persistentvolumes
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "512Mi"
      volumes:
        - name: nfs-volume
          nfs:
            server: 192.168.1.233
            path: /data/nfs
            readOnly: false
```

```sh
kubectl apply -f nfs-provisioner.yaml
```

创建 StorageClass

> StorageClass：只能新建，不能修改,当有其他应用在用时不要删旧的，要建新的策略，删旧的会导致数据丢失

```yaml
# nfs-storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-client
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
parameters:
  archiveOnDelete: "true"
reclaimPolicy: Retain
volumeBindingMode: Immediate
```

```sh
kubectl apply -f nfs-storageclass.yaml
```

（可选）设为默认 StorageClass

```sh
kubectl patch storageclass nfs-client \
  -p '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
```

验证（最重要）

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-nfs-pvc
  namespace: nfs-storage   # 指定命名空间
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: nfs-client
  resources:
    requests:
      storage: 1Gi
```

```sh
kubectl apply -f pvc.yaml
kubectl get pvc
```

重启Deployment

```sh
kubectl rollout restart deploy nfs-client-provisioner -n nfs-storage
```

测试pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nfs-test-pod
  namespace: nfs-storage
spec:
  containers:
  - name: test
    image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/registry.k8s.io/e2e-test-images/busybox:1.29-4
    command: ["sleep", "3600"]
    volumeMounts:
    - mountPath: "/mnt/nfs"
      name: nfs-volume
  volumes:
  - name: nfs-volume
    persistentVolumeClaim:
      claimName: test-nfs-pvc
```

进入pod

```sh
kubectl exec -it -n nfs-storage nfs-test-pod -- /bin/sh
```

进入之后，你可以在 PVC 挂载路径里创建文件。例如，如果 PVC 挂载在 `/mnt/nfs`：

```sh
cd /mnt/nfs
echo "hello nfs" > test-file.txt
cat test-file.txt
```

创建第二个pod，测试文件是否能共用

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nfs-test-pod2
  namespace: nfs-storage
spec:
  containers:
    - name: app
      image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/registry.k8s.io/e2e-test-images/busybox:1.29-4
      command: ["/bin/sh", "-c", "sleep 3600"]
      volumeMounts:
        - name: nfs-volume
          mountPath: "/mnt/nfs"
  volumes:
    - name: nfs-volume
      persistentVolumeClaim:
        claimName: test-nfs-pvc
```

```sh
kubectl apply -f nfs-test-pod2.yaml
```

```sh
kubectl exec -it -n nfs-storage nfs-test-pod2 -- /bin/sh
cd /mnt
cat test-file.txt   # 应该能看到 "hello nfs"
echo "hello again" >> test-file.txt
```

## 安装argo CD

### 下载安装文件

```sh
https://raw.githubusercontent.com/argoproj/argo-cd/v3.0.6/manifests/install.yaml
#修改容器源地址
quay.io/argoproj/argocd
#替换成
swr.cn-north-4.myhuaweicloud.com/ddn-k8s/quay.io/argoproj/argocd:v3.0.6

ghcr.io/dexidp/dex:v2.43.0
#替换成
swr.cn-north-4.myhuaweicloud.com/ddn-k8s/ghcr.io/dexidp/dex:v2.43.0
```

### 修改访问方式

```yaml
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
    nodePort: 30081
  - name: https
    port: 443
    protocol: TCP
    targetPort: 8080
    nodePort: 30080
  selector:
    app.kubernetes.io/name: argocd-server
```

### 安装运行

```sh
#创建命名空间
kubectl create namespace argocd
#运行命令
kubectl apply -n argocd -f ArgoCD-install.yaml
#删除
kubectl delete -n argocd -f ArgoCD-install.yaml
```

### 获取密码

```sh
#取初始 admin 密码（登录之后记得改密码）
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d && echo
```

### 访问

```http
https://192.168.1.230:30081/
```

## 国内镜像地址

```http
https://docker.aityp.com/
```

## 安装xxl-job

### Application

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: xxl-job-admin
  namespace: argocd
spec:
  project: default

  source:
    repoURL: http://192.168.1.142/root/devops.git
    targetRevision: main
    path: charts/xxl-job-admin   # 你放的 chart 目录
    helm:
      releaseName: xxl-job-admin
      valueFiles:
        - values.yaml

  destination:
    server: https://kubernetes.default.svc
    namespace: mes  # 空间名称

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
```

### values.yaml

```yaml
replicaCount: 1
image:
  repository: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/xuxueli/xxl-job-admin
  tag: 3.2.0
  pullPolicy: Always

service:
  type: NodePort #外部访问
  port: 8080
  nodePort: 30808

mysql:
  host: 192.168.1.221
  port: 3306
  database: xxl_job
  user: root
  password: "123456"

xxl:
  accessToken: "hzwx"
```

### Chart.yaml

```yaml
apiVersion: v2
name: xxl-job-admin
description: A Helm chart for Kubernetes
type: application
version: 0.1.0
appVersion: "1.16.0"
```

### deployment.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: xxl-job-admin
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: xxl-job-admin
  template:
    metadata:
      labels:
        app: xxl-job-admin
    spec:
      containers:
        - name: xxl-job-admin
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - containerPort: 8080
          env:
            - name: SPRING_DATASOURCE_URL
              value: "jdbc:mysql://{{ .Values.mysql.host }}:{{ .Values.mysql.port }}/{{ .Values.mysql.database }}?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&serverTimezone=Asia/Shanghai"
            - name: SPRING_DATASOURCE_USERNAME
              value: "{{ .Values.mysql.username }}"
            - name: SPRING_DATASOURCE_PASSWORD
              value: "{{ .Values.mysql.password }}"
            - name: XXL_JOB_ACCESS_TOKEN
              value: "{{ .Values.xxl.accessToken }}"
```

### service.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: xxl-job-admin
spec:
  type: NodePort
  ports:
    - port: {{ .Values.service.port }}     # Service 内部端口
      targetPort: 8080                     # 容器端口
      nodePort: {{ .Values.service.nodePort }}   # 外部访问端口
  selector:
    app: xxl-job-admin
```

### 安装3.2.0需要修改表结构

> 安装一定要看官方文档，特别是所安装的版本更新信息

```mysql
  // 1、用户表password字段需要调整长度，执行如下命令
  ALTER TABLE xxl_job_user
      MODIFY COLUMN `password` varchar(100) NOT NULL COMMENT '密码加密信息';
  ALTER TABLE xxl_job_user
      ADD COLUMN `token` varchar(100) DEFAULT NULL COMMENT '登录token';
  // 2、存量用户密码需要修改，可执行如下命令将密码初始化 “123456”；也可以自行通过 “SHA256Tool.sha256” 工具生成其他初始化密码；
  UPDATE xxl_job_user t SET t.password = '8d969eef6ecad3c29a3a629280e686cf0c3f5d5a86aff3ca12020c923adc6c92' WHERE t.username = {用户名};
```

## 安装rocketmq

**官网**：https://rocketmq.apache.org/zh/

### Application.yaml

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: rocketmq
  namespace: argocd
spec:
  project: default

  source:
    repoURL: http://192.168.1.142/root/devops.git
    targetRevision: main
    path: charts/rocketmq

    helm:
      releaseName: rocketmq
      valueFiles:
        - values.yaml

  destination:
    server: https://kubernetes.default.svc
    namespace: mes

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
```

### Chart.yaml

```yaml
apiVersion: v2
name: rocketmq
description: RocketMQ 5.3.2 Offline Helm Chart (SWR)
type: application
version: 1.0.0
appVersion: "5.3.2"
```

### values.yaml

```yaml
# 存储配置
storage:
  enabled: true
  size: 200Gi  # 配置存储容量
  storageClass: nfs-client  # 使用 NFS 存储

# NameServer 配置
nameserver:
  replicaCount: 2  # 部署 2 个 NameServer 实例
  servicePort: 9876  # 服务端口
  image:
    repository: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/apache/rocketmq
    tag: 5.3.2  # 镜像版本
    pullPolicy: IfNotPresent  # 如果本地没有镜像，则从仓库拉取

# Broker 配置
broker:
  replicaCount: 2  # 部署 2 个 Broker 实例，建议高可用性配置
  image:
    repository: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/apache/rocketmq
    tag: 5.3.2  # 镜像版本
    pullPolicy: IfNotPresent  # 如果本地没有镜像，则从仓库拉取
  brokerClusterName: DefaultCluster  # 默认集群名称
  brokerName: broker-a  # Broker 名称
  brokerId: 0  # Broker ID
  brokerRole: ASYNC_MASTER  # Broker 角色
  flushDiskType: ASYNC_FLUSH  # 刷盘类型

  # 持久化配置
  persistence:
    enabled: true  # 启用持久化
    storageClass: nfs-client  # 使用 NFS 存储类
    size: 50Gi  # 配置存储容量

  jvmOptions: "-Xms2g -Xmx2g -Xmn1g"  # 配置 JVM 内存选项

# Dashboard 配置
dashboard:
  enabled: true  # 启用 RocketMQ Dashboard
  image:
    repository: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/apacherocketmq/rocketmq-dashboard
    tag: latest  # Dashboard 镜像版本
  serviceType: NodePort  # 使用 NodePort 类型服务
  nodePort: 30888  # 外部访问的端口
  username: admin  # Dashboard 登录用户名
  password: admin123  # Dashboard 登录密码
```

### _helpers.tpl

```tex
{{/*
Expand the name of the chart.
*/}}
{{- define "rocketmq.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Create a default fully qualified app name.
If release name contains chart name it will be used as a full name.
*/}}
{{- define "rocketmq.fullname" -}}
{{- if .Values.fullnameOverride }}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- $name := default .Chart.Name .Values.nameOverride }}
{{- if contains $name .Release.Name }}
{{- .Release.Name | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" }}
{{- end }}
{{- end }}
{{- end }}

{{/*
Create chart name and version as used by the chart label.
*/}}
{{- define "rocketmq.chart" -}}
{{- printf "%s-%s" .Chart.Name .Chart.Version | replace "+" "_" | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Common labels
*/}}
{{- define "rocketmq.labels" -}}
helm.sh/chart: {{ include "rocketmq.chart" . }}
{{ include "rocketmq.selectorLabels" . }}
{{- if .Chart.AppVersion }}
app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}
{{- end }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- end }}

{{/*
Selector labels
*/}}
{{- define "rocketmq.selectorLabels" -}}
app.kubernetes.io/name: {{ include "rocketmq.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
{{- end }}

{{/*
Create the name of the service account to use
*/}}
{{- define "rocketmq.serviceAccountName" -}}
{{- if .Values.serviceAccount.create }}
{{- default (include "rocketmq.fullname" .) .Values.serviceAccount.name }}
{{- else }}
{{- default "default" .Values.serviceAccount.name }}
{{- end }}
{{- end }}

{{/*
RocketMQ NameServer address (for broker / dashboard)
*/}}
{{- define "rocketmq.namesrvAddr" -}}
{{ include "rocketmq.fullname" . }}-nameserver-svc.{{ .Release.Namespace }}.svc.cluster.local:9876
{{- end }}
```

### broker-configmap.yaml

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "rocketmq.fullname" . }}-broker-config
  labels:
    {{- include "rocketmq.labels" . | nindent 4 }}
data:
  broker.conf: |
    brokerClusterName={{ .Values.broker.brokerClusterName | default "DefaultCluster" }}
    brokerName={{ .Values.broker.brokerName | default "broker-a" }}
    brokerId={{ .Values.broker.brokerId | default 0 }}
    brokerRole={{ .Values.broker.brokerRole | default "ASYNC_MASTER" }}
    flushDiskType={{ .Values.broker.flushDiskType | default "ASYNC_FLUSH" }}
    namesrvAddr={{ include "rocketmq.namesrvAddr" . }}
    autoCreateTopicEnable=false
    autoCreateSubscriptionGroup=false
```

### broker-service.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: {{ include "rocketmq.fullname" . }}-broker
  labels:
    {{- include "rocketmq.labels" . | nindent 4 }}
spec:
  clusterIP: None
  selector:
    {{- include "rocketmq.selectorLabels" . | nindent 6 }}
  ports:
    - port: 10911
      targetPort: 10911
```

### broker-statefulset.yaml

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ include "rocketmq.fullname" . }}-broker
  labels:
    {{- include "rocketmq.labels" . | nindent 4 }}
spec:
  serviceName: {{ include "rocketmq.fullname" . }}-broker
  replicas: {{ .Values.broker.replicaCount }}
  selector:
    matchLabels:
      {{- include "rocketmq.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "rocketmq.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      containers:
        - name: broker
          image: "{{ .Values.broker.image.repository }}:{{ .Values.broker.image.tag }}"
          imagePullPolicy: {{ .Values.broker.image.pullPolicy }}
          command:
            - sh
            - -c
            - /home/rocketmq/rocketmq-5.3.2/bin/mqbroker -c /home/rocketmq/broker.conf
          env:
            - name: JAVA_OPT_EXT
              value: "{{ .Values.broker.jvmOptions }}"
            - name: ROCKETMQ_NAMESRV_ADDR
              value: "{{ include "rocketmq.namesrvAddr" . }}"
          ports:
            - containerPort: 10911
            - containerPort: 10909
          volumeMounts:
            - name: conf
              mountPath: /home/rocketmq/broker.conf
              subPath: broker.conf
            - name: data
              mountPath: /home/rocketmq/store
          readinessProbe:
            tcpSocket:
              port: 10911
            initialDelaySeconds: 30
            periodSeconds: 10
          livenessProbe:
            tcpSocket:
              port: 10911
            initialDelaySeconds: 60
            periodSeconds: 10
          resources:
            requests:
              memory: "4Gi"
              cpu: "2"
            limits:
              memory: "6Gi"
              cpu: "2"
      volumes:
        - name: conf
          configMap:
            name: {{ include "rocketmq.fullname" . }}-broker-config
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: {{ .Values.broker.persistence.storageClass }}
        resources:
          requests:
            storage: {{ .Values.broker.persistence.size }}
```

### dashboard-configmap.yaml

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "rocketmq.fullname" . }}-dashboard-config
  namespace: {{ .Release.Namespace }}
data:
  application.yml: |
    server:
      port: 8080
    rocketmq:
      config:
        loginRequired: true
        dataPath: /tmp/rocketmq-console
  users.properties: |
    {{ .Values.dashboard.username | quote }}={{ .Values.dashboard.password | quote }},1
```

### dashboard-deployment.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "rocketmq.fullname" . }}-dashboard
  labels:
    {{ include "rocketmq.labels" . | nindent 4 }}
spec:
  replicas: 1
  selector:
    matchLabels:
      {{ include "rocketmq.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{ include "rocketmq.selectorLabels" . | nindent 8 }}
    spec:
      imagePullSecrets:
        {{ toYaml .Values.imagePullSecrets | indent 8 }}
      containers:
        - name: dashboard
          image: "{{ .Values.dashboard.image.repository }}:{{ .Values.dashboard.image.tag }}"
          env:
            - name: ROCKETMQ_NAMESRV_ADDR
              value: "192.168.1.230:30076"
            - name: JAVA_OPTS
              value: "-Drocketmq.namesrv.addr=192.168.1.230:30076 -Drocketmq.config.loginRequired=true"
          ports:
            - containerPort: 8080
          volumeMounts:
            - name: dashboard-config
              mountPath: /opt/rocketmq-dashboard/application.yml
              subPath: application.yml
            - name: dashboard-config
              mountPath: /opt/rocketmq-dashboard/users.properties
              subPath: users.properties
      volumes:
        - name: dashboard-config
          configMap:
            name: {{ include "rocketmq.fullname" . }}-dashboard-config
```

### dashboard-service.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: {{ include "rocketmq.fullname" . }}-dashboard
  namespace: {{ .Release.Namespace }}
spec:
  type: {{ .Values.dashboard.serviceType | default "NodePort" }}
  selector:
    app.kubernetes.io/name: {{ include "rocketmq.name" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}
  ports:
    - name: http
      port: 8080
      targetPort: 8080
      nodePort: {{ .Values.dashboard.nodePort | default 30888 }}
```

### nameserver-configmap.yaml

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "rocketmq.fullname" . }}-nameserver-config
  labels:
    {{- include "rocketmq.labels" . | nindent 4 }}
data:
  namesrv.conf: |
    listenPort={{ .Values.nameserver.servicePort }}
    storePathRootDir=/home/rocketmq/store/namesrv
    logLevel=INFO
    logFile=/home/rocketmq/store/namesrv/logs/namesrv.log
    brokerClusterChangeInterval=30000
    autoCreateTopicEnable=false
    autoCreateSubscriptionGroup=false
```

### nameserver-service.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: rocketmq-nameserver-svc
  namespace: mes
spec:
  type: NodePort  # 或者 LoadBalancer
  ports:
    - name: namesrv
      port: 9876
      targetPort: 9876
      nodePort: 30076  # 例如，分配一个 NodePort
  selector:
    app.kubernetes.io/name: rocketmq
    app.kubernetes.io/instance: {{ .Release.Name }}
```

### nameserver-statefulset.yaml

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ include "rocketmq.fullname" . }}-nameserver
  labels:
    app.kubernetes.io/name: {{ include "rocketmq.name" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}
spec:
  serviceName: {{ include "rocketmq.fullname" . }}-nameserver-svc
  replicas: {{ .Values.nameserver.replicaCount }}
  selector:
    matchLabels:
      app.kubernetes.io/name: {{ include "rocketmq.name" . }}
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: {{ include "rocketmq.name" . }}
        app.kubernetes.io/instance: {{ .Release.Name }}
    spec:
      imagePullSecrets:
        {{ toYaml .Values.imagePullSecrets | indent 8 }}
      containers:
        - name: nameserver
          image: "{{ .Values.nameserver.image.repository }}:{{ .Values.nameserver.image.tag }}"
          imagePullPolicy: {{ .Values.nameserver.image.pullPolicy }}
          command:
            - sh
            - -c
            - /home/rocketmq/rocketmq-5.3.2/bin/mqnamesrv
          env:
            - name: JAVA_OPT_EXT
              value: "-Xms512m -Xmx512m"
          ports:
            - containerPort: {{ .Values.nameserver.servicePort }}
          volumeMounts:
            - name: conf
              mountPath: /home/rocketmq/conf
          resources:
            requests:
              memory: "512Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1"
      volumes:
        - name: conf
          configMap:
            name: {{ include "rocketmq.fullname" . }}-nameserver-config
```

## 安装cert-manager

### 添加仓库

```sh
helm repo add jetstack https://charts.jetstack.io
helm repo update
```

### 安装

```sh
#有哪些仓库
helm search repo
#查看版本
helm search repo cert-manager --versions
#手动安装CRDs（一定要先安装）
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.19.2/cert-manager.crds.yaml

# 3. 安装 cert-manager
helm install cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --version v1.18.2
  
  #安装cert-manager时，自动安装CRDs（老版本）
helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --set installCRDs=true

#卸载
helm uninstall cert-manager --namespace cert-manager
#卸载CRDs
kubectl delete crds certificates.cert-manager.io issuers.cert-manager.io clusterissuers.cert-manager.io
#新版本安装
helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --set crds.enabled=true
```

1. 从 `cert-manager` v1.18 版本开始，`Certificate` 资源的私钥轮换策略默认设置为 `Always`，如果你需要特定的私钥轮换策略，可以在 `Certificate` 资源中设置。

2. **后续配置**：

   ```
   In order to begin issuing certificates, you will need to set up a ClusterIssuer or Issuer resource.
   ```

   你需要配置一个 **ClusterIssuer** 或 **Issuer** 资源，以开始颁发证书。

### 后续步骤：

#### 1. **创建 `Issuer` 或 `ClusterIssuer`**

如果你打算使用 **Let's Encrypt** 来颁发证书，你可以创建一个 **ClusterIssuer**。以下是一个使用 Let's Encrypt Staging 环境的例子：

```yaml
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-staging
spec:
  acme:
    server: https://acme-staging-v02.api.letsencrypt.org/directory
    email: your-email@example.com
    privateKeySecretRef:
      name: letsencrypt-staging-key
    solvers:
    - http01:
        ingress: {}
```

保存为 `clusterissuer.yaml`，然后应用：

```sh
kubectl apply -f clusterissuer.yaml
```

#### 2. **创建 `Certificate` 资源进行证书请求**

创建一个 **Certificate** 资源，用于申请证书：

```yaml
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: example-tls
  namespace: default
spec:
  secretName: example-tls-secret
  issuerRef:
    name: letsencrypt-staging
    kind: ClusterIssuer
  dnsNames:
    - "example.com"
```

保存为 `certificate.yaml` 并应用：

```sh
kubectl apply -f certificate.yaml
```

#### 3. **验证证书**

可以使用以下命令检查 **Certificate** 资源的状态：

```sh
kubectl get certificate example-tls -n default
```

确保证书的状态为 `True`，并且证书资源创建成功。

#### 4. **检查 `cert-manager` 的 Pod 状态**

你可以通过以下命令检查 `cert-manager` 的 Pod 是否正常运行：

```sh
kubectl get pods -n cert-manager
```

确保所有相关 Pod 都是 `Running` 状态。

#### 创建新的证书

```sh
kubectl create secret tls abc-com-tls \
  --cert=_.abc.com.crt \
  --key= \
  -n mes
```

#### 更新证书

```sh
kubectl create secret tls abc-com-tls \
  --cert=new-fullchain.pem \
  --key=new-server.key \
  -n your-namespace \
  --dry-run=client -o yaml | kubectl apply -f -
```

## 安装MetalLB(负载均衡)

### 官网

https://metallb.io/

### 添加仓库helm

```sh
helm repo add metallb https://metallb.github.io/metallb
helm repo update
```

### 安装

```sh
# 安装 metallb 到 metallb-system 命名空间
helm install metallb metallb/metallb -n metallb-system --create-namespace
```

### 配置 IPAddressPool

> 新建一个 `IPAddressPool`，告诉 MetalLB 可以用哪些 IP 来分配给 `Service type=LoadBalancer`

```yaml
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: default-address-pool
  namespace: metallb-system
spec:
  addresses:
  - 192.168.1.240-192.168.1.250 #分配的IP，分配之后这些IP不能被其他设备使用
```

### 配置 L2Advertisement

> 让 MetalLB 广播这段 IP

```yaml
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: default
  namespace: metallb-system
spec:
  ipAddressPools:
  - default-address-pool
```

### 验证是否安装成功

```yaml
apiVersion: v1
kind: Service
metadata:
  name: test-nginx
spec:
  type: LoadBalancer
  selector:
    app: nginx
  ports:
    - port: 80
      targetPort: 80
```

### 查看IP是否分配成功

```sh
kubectl get svc test-nginx
```

### 成功分配

```sh
mes@masters:~$ kubectl get svc test-nginx
NAME         TYPE           CLUSTER-IP    EXTERNAL-IP       PORT(S)        AGE
test-nginx   LoadBalancer   10.43.70.81   192.168.1.234   80:31710/TCP   33s
```

### 删除测试

```sh
kubectl delete -f test-nginx.yaml
```

### 查看资源

```sh
kubectl get crds | grep metallb
#查看地址池
kubectl get ipaddresspools -n metallb-system
```

## Traefik

### 创建命名空间

```sh
kubectl create namespace traefik
```

### 添加 Traefik Helm 仓库

```sh
helm repo add traefik https://helm.traefik.io/traefik
helm repo update
```

### 安装(网络能正常访问docker.io的情况下使用这个方法)

```sh
#不启用版本
helm install traefik traefik/traefik --namespace traefik
#启用Dashboard版本(生产环境不推荐启用)
helm install traefik traefik/traefik --namespace traefik --set dashboard.enabled=true
#指定版本和修改镜像地址
helm install traefik traefik/traefik --namespace traefik --version 37.4.0 -f traefik-values.yaml
```

### 解决自动带有docker.io/前缀的办法

```shell
# 1. 生成模板
helm template traefik traefik/traefik \
  -n traefik \
  --version 37.4.0 \
  -f traefik.yaml \
  > traefik-rendered.yaml
  
#这个是国内镜像
swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/traefik:v3.6.5
# 2. 手动修复镜像地址 
sed -i 's|docker.io/swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/traefik:v3.6.5|swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/traefik:v3.6.5|g' traefik.yaml

# 3. 直接应用
kubectl apply -f traefik-rendered.yaml
# 4.删除
kubectl delete -f traefik-rendered.yaml
```

### traefik-rendered.yaml

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: traefik
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: traefik
  namespace: traefik
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: traefik
rules:
  - apiGroups: [""]
    resources:
      - services
      - endpoints
      - secrets
      - pods
      - nodes
    verbs:
      - get
      - list
      - watch
  - apiGroups: ["networking.k8s.io", "extensions"]
    resources:
      - ingresses
      - ingressclasses
    verbs:
      - get
      - list
      - watch
  - apiGroups: ["traefik.io"]
    resources:
      - ingressroutes
      - ingressroutetcps
      - ingressrouteudps
      - middlewares
      - middlewaretcps
      - tlsoptions
      - tlsstores
      - traefikservices
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: traefik
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: traefik
subjects:
  - kind: ServiceAccount
    name: traefik
    namespace: traefik
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: traefik
  namespace: traefik
spec:
  replicas: 2
  selector:
    matchLabels:
      app: traefik
  template:
    metadata:
      labels:
        app: traefik
    spec:
      serviceAccountName: traefik
      securityContext:
        runAsUser: 65532
        runAsGroup: 65532
        runAsNonRoot: true
      containers:
        - name: traefik
          image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/traefik:v3.6.5
          imagePullPolicy: IfNotPresent
          args:
            - "--entryPoints.web.address=:8000/tcp"
            - "--entryPoints.websecure.address=:8443/tcp"
            - "--entryPoints.traefik.address=:8080/tcp"
            - "--providers.kubernetescrd=true"
            - "--providers.kubernetesingress=false"
            - "--entryPoints.websecure.http.tls=true"
            - "--log.level=INFO"
            - "--accesslog=true"
            - "--ping=true"
          ports:
            - name: web
              containerPort: 8000
            - name: websecure
              containerPort: 8443
            - name: admin
              containerPort: 8080
          readinessProbe:
            httpGet:
              path: /ping
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /ping
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: traefik
  namespace: traefik
spec:
  type: LoadBalancer
  selector:
    app: traefik
  ports:
    - name: web
      port: 80
      targetPort: 8000
    - name: websecure
      port: 443
      targetPort: 8443
---
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  name: traefik
  annotations:
    ingressclass.kubernetes.io/is-default-class: "true"
spec:
  controller: traefik.io/ingress-controller
```

### 设置容器

```yaml
images:
  proxy:
    image: traefik
    tag: v3.6.2
    registry: swr.cn-north-4.myhuaweicloud.com/ddn-k8s
  hub:
    image: traefik-hub
    tag: latest
    registry: swr.cn-north-4.myhuaweicloud.com/ddn-k8s
```

### 卸载

```sh
helm uninstall traefik --namespace traefik
#查看是否已经卸载完成
kubectl get all -n traefik
```

### 配置 Ingress 资源

创建一个简单的 Ingress 配置来验证 Traefik 的工作：

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
  namespace: default
  annotations:
    traefik.ingress.kubernetes.io/router.entrypoints: web
spec:
  rules:
  - host: myapp.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: myapp-service
            port:
              number: 80
```

然后创建一个对应的 Service 以供 Traefik 路由流量到您的应用。

### 配置 Traefik Dashboard（可选）

要启用 Traefik Dashboard，可以在安装时通过 `values.yaml` 配置，或者单独应用如下配置：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: traefik-dashboard
  namespace: traefik
spec:
  replicas: 1
  selector:
    matchLabels:
      app: traefik-dashboard
  template:
    metadata:
      labels:
        app: traefik-dashboard
    spec:
      containers:
        - name: traefik-dashboard
          image: traefik:v2.5
          args:
            - --api.insecure=true
            - --entryPoints.web.address=:80
            - --entryPoints.websecure.address=:443
```

访问 Dashboard 时，需要配置一个安全访问方式（如认证）。

### 5. 访问 Traefik

通过外部 IP 或域名访问 Traefik 路由的应用。如果你启用了 `LoadBalancer` 类型的 Service，可以通过获取外部 IP 来访问。

```sh
kubectl get svc -n traefik
```

这将展示 Traefik 服务的外部 IP 地址，您可以用它来访问应用。

## MES后端

### 项目镜像打包

```dockerfile
FROM eclipse-temurin:8-jre

## 创建目录，并使用它作为工作目录
RUN mkdir -p /yudao-server /yudao-server/csvfile
WORKDIR /yudao-server
## 将后端项目的 Jar 文件，复制到镜像中
COPY yudao-server.jar app.jar

# 设置时区
ENV TZ=Asia/Shanghai

# 16G内存优化配置
ENV JAVA_OPTS="-Xms16g -Xmx16g \
               -XX:+UseG1GC \
               -XX:NewRatio=2 \
               -XX:SurvivorRatio=8 \
               -XX:MaxGCPauseMillis=200 \
               -XX:InitiatingHeapOccupancyPercent=45 \
               -XX:G1ReservePercent=15 \
               -XX:+PrintGC \
               -XX:+PrintGCDateStamps \
               -Xloggc:/yudao-server/gc.log \
               -XX:+HeapDumpOnOutOfMemoryError \
               -XX:HeapDumpPath=/yudao-server/heapdump.hprof"
# 暴露端口
EXPOSE 48080
# 启动程序
CMD java ${JAVA_OPTS} -Djava.security.egd=file:/dev/./urandom -jar app.jar
```

### jenkins做CI部分代码

> 这个部分可以gitlab来做

```groovy
#!groovy
pipeline {
    //指定任务在那个集群节点
    agent any
    environment {
        //声明全局变量，方便后使用
        harborUser = 'admin'
        harborPasswd = '123456'
        harborAddress = '192.168.1.166:80'
        harborRepo = 'mes-server'
    }
    //全局指定
    tools {
        jdk 'jdk-8u381'
        maven 'maven3.9'
    }

    stages {

        stage('拉取git代码') {
            steps {
                //checkout scmGit(branches: [[name: '${tag}']], extensions: [], userRemoteConfigs: [[url: 'http://192.168.1.142/root/mytest.git']])
                checkout scmGit(branches: [[name: '${tag}']], extensions: [], userRemoteConfigs: [[credentialsId: 'b1f83372-2a32-4a6c-bf34-5238a268', url: 'http://192.168.1.142/root/hzwx.git']])
            }
        }
        stage('验证 JDK') {
            steps {
                sh 'java -version'
            }
        }
        stage('通过maven构建项目') {
            steps {
                //sh '/var/jenkins_home/maven/bin/mvn package -DskipTest'
                sh '/var/jenkins_home/maven/bin/mvn  clean package -Dmaven.test.skip=true'
            }
        }

        stage('通过sonarQube代码质量检测') {
            steps {
                sh '/var/jenkins_home/sonar-scanner/bin/sonar-scanner  -Dsonar.source=./  -Dsonar.projectname=${JOB_NAME}  -Dsonar.projectKey=${JOB_NAME}  -Dsonar.java.binaries=/var/jenkins_home/workspace/prod/yudao-server/target  -Dsonar.login=sqa_85799e499f7e7221f31907285ef133c9'
            }
        }

        stage('通过docker制作自定义镜像') {
            steps {
                sh '''mv /var/jenkins_home/workspace/k8s-prod/yudao-server/target/*.jar ./yudao-server/
                     docker build -t  yudao-server:${tag} /var/jenkins_home/workspace/k8s-prod/yudao-server'''
            }
        }
        stage('将自定义镜像推送到Harbor仓库') {
            steps {
                sh '''docker login -u ${harborUser} -p ${harborPasswd} ${harborAddress}
                           docker tag yudao-server:${tag} 192.168.1.166:80/mes-server/yudao-server:${tag}
                           docker push ${harborAddress}/${harborRepo}/yudao-server:${tag}'''
            }
        }
        stage('更新 DevOps 仓库中的后端镜像版本') {
            steps {
                withCredentials([
                        usernamePassword(
                                credentialsId: 'devops-git-cred',
                                usernameVariable: 'GIT_USER',
                                passwordVariable: 'GIT_PASS'
                        )
                ]) {
                    script {
                        // 对密码进行 URL 编码
                        ENCODED_PASS = java.net.URLEncoder.encode(GIT_PASS, "UTF-8")
                    }
                    sh """
                echo "===== 克隆 DevOps 仓库 ====="
                rm -rf devops-repo

                # 指定 Jenkins 的 HOME 目录
                export HOME=/var/jenkins_home

                # 删除旧的不属于 Jenkins 的文件
                rm -f \$HOME/.git-credentials

                # 写入 GitLab 凭证（使用编码后的密码）
                echo "http://${GIT_USER}:${ENCODED_PASS}@192.168.1.142" > \$HOME/.git-credentials
                chmod 600 \$HOME/.git-credentials

                git config --global credential.helper store

                # 测试一下 Git 是否能读取凭证
                git config --global --list

                git clone http://192.168.1.142/root/devops.git devops-repo
                cd devops-repo

                echo "===== 切换分支 ====="
                git checkout main

                echo "===== 修改 mes-server 的 Helm values.yaml ====="
                sed -i "s/tag: .*/tag: ${tag}/" ./charts/mes-server/values.yaml

                echo "===== 配置 Git 用户 ====="
                git config user.name "Administrator"
                git config user.email "admin@example.com"

                echo "===== 提交并 push ====="
                git add .
                git commit -m "Update mes-server image tag to ${tag}" || echo "No changes to commit"
                git push origin main
            """
                }
            }
        }
    }
    post {
        success {
            dingtalk(
                    robot: 'prod-k8s',
                    type: 'MARKDOWN',
                    title: "success: ${JOB_NAME}",
                    text: ["- 成功构建:${JOB_NAME}项目!\n- 版本:${tag}\n- 持续时间:${currentBuild.durationString}\n- 任务:#${JOB_NAME}"]
            )
        }
        failure {
            dingtalk(
                    robot: 'prod-k8s',
                    type: 'MARKDOWN',
                    title: "fail: ${JOB_NAME}",
                    text: ["- 失败构建:${JOB_NAME}项目!\n- 版本:${tag}\n- 持续时间:${currentBuild.durationString}\n- 任务:#${JOB_NAME}"]
            )
        }
    }

}
```

### Argo CD做CD部分

#### Application.yaml

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: mes-server
  namespace: argocd  # ArgoCD 安装的 namespace，通常是 argocd
spec:
  project: default

  source:
    repoURL: http://192.168.1.142/root/devops.git
    targetRevision: main
    path: charts/mes-server  # 你的 helm chart 路径
    helm:
      releaseName: mes-server
      valueFiles:
        - values.yaml

  destination:
    server: https://kubernetes.default.svc
    namespace: mes  # 你的应用部署到的 namespace

  syncPolicy:
    automated:
      prune: true          # 自动删除不需要的资源
      selfHeal: true       # 自动修复偏移
    syncOptions:
      - CreateNamespace=true
```

#### Chart.yaml

```yaml
apiVersion: v2
name: mes-server
description: MES Java Backend Service
type: application
version: 0.1.0
appVersion: "1.0"
```

#### values.yaml

```yaml
# ==============================
# 副本数
# ==============================
replicaCount: 3

# ==============================
# 镜像配置
# ==============================
image:
  repository: 192.168.1.166:80/mes-server/yudao-server
  tag: v4.2.14
  pullPolicy: Always

# ==============================
# 容器端口
# ==============================
containerPorts:
  http: 48080
  printer: 9111

# ==============================
# 环境变量
# ==============================
env:
  - name: SPRING_PROFILES_ACTIVE
    value: prod
  - name: JAVA_OPTS
    value: "-Xms512m -Xmx1536m -XX:+UseG1GC -Dfile.encoding=UTF-8"
  # RocketMQ NameServer（关键）
  - name: ROCKETMQ_NAME_SERVER
    value: rocketmq-nameserver-svc.mes.svc.cluster.local:9876

  # XXL-JOB（内网场景：不启用 token，必须和 Admin 一致）
  - name: XXL_JOB_ACCESS_TOKEN
    value: ""

# ==============================
# Pod 资源限制
# ==============================
resources:
  limits:
    cpu: 2000m
    memory: 2Gi
  requests:
    cpu: 500m
    memory: 1Gi

# ==============================
# 健康检查
# ==============================
livenessProbe:
  enabled: true
  path: /actuator/health
  initialDelaySeconds: 180
  timeoutSeconds: 5
  periodSeconds: 30
  failureThreshold: 5

readinessProbe:
  enabled: true
  path: /actuator/health
  initialDelaySeconds: 120
  timeoutSeconds: 3
  periodSeconds: 15
  failureThreshold: 3

# ==============================
# 服务配置
# ==============================
serviceHttp:
  type: ClusterIP
  port: 48080

servicePrinter:
  type: NodePort
  port: 9111
  nodePort: 30911

# ==============================
# Ingress 配置（只走 HTTP 48080）
# ==============================
ingress:
  enabled: true
  hosts:
    - kmes.abc.com
    - kapp.abc.com
  tls:
    secretName: abc-com-tls
```

#### _helpers.tpl

```yaml
{{- define "mes-server.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" }}
{{- end }}

{{- define "mes-server.fullname" -}}
{{- if .Values.fullnameOverride }}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- $name := default .Chart.Name .Values.nameOverride }}
{{- if contains $name .Release.Name }}
{{- .Release.Name | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" }}
{{- end }}
{{- end }}
{{- end }}

{{- define "mes-server.chart" -}}
{{- printf "%s-%s" .Chart.Name .Chart.Version | trunc 63 | trimSuffix "-" }}
{{- end }}

{{- define "mes-server.labels" -}}
helm.sh/chart: {{ include "mes-server.chart" . }}
app.kubernetes.io/name: {{ include "mes-server.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- end }}

{{- define "mes-server.selectorLabels" -}}
app.kubernetes.io/name: {{ include "mes-server.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
{{- end }}
```

#### deployment.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "mes-server.fullname" . }}
  labels:
    {{- include "mes-server.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.replicaCount }}
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 50%
      maxUnavailable: 50%
  selector:
    matchLabels:
      {{- include "mes-server.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "mes-server.selectorLabels" . | nindent 8 }}
    spec:
      containers:
        - name: {{ .Chart.Name }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}

          ports:
            - name: http
              containerPort: {{ .Values.containerPorts.http }}
            - name: printer
              containerPort: {{ .Values.containerPorts.printer }}

          env:
            {{- toYaml .Values.env | nindent 12 }}

          resources:
            {{- toYaml .Values.resources | nindent 12 }}

          livenessProbe:
            httpGet:
              path: {{ .Values.livenessProbe.path }}
              port: {{ .Values.containerPorts.http }}
            initialDelaySeconds: {{ .Values.livenessProbe.initialDelaySeconds }}
            timeoutSeconds: {{ .Values.livenessProbe.timeoutSeconds }}
            periodSeconds: {{ .Values.livenessProbe.periodSeconds }}
            successThreshold: {{ .Values.livenessProbe.successThreshold }}
            failureThreshold: {{ .Values.livenessProbe.failureThreshold }}

          readinessProbe:
            httpGet:
              path: {{ .Values.readinessProbe.path }}
              port: {{ .Values.containerPorts.http }}
            initialDelaySeconds: {{ .Values.readinessProbe.initialDelaySeconds }}
            timeoutSeconds: {{ .Values.readinessProbe.timeoutSeconds }}
            periodSeconds: {{ .Values.readinessProbe.periodSeconds }}
            successThreshold: {{ .Values.readinessProbe.successThreshold }}
            failureThreshold: {{ .Values.readinessProbe.failureThreshold }}
```

#### service-http.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: {{ include "mes-server.fullname" . }}-http
spec:
  type: {{ .Values.serviceHttp.type }}
  selector:
    {{- include "mes-server.selectorLabels" . | nindent 4 }}
  ports:
    - name: http
      port: {{ .Values.serviceHttp.port }}
      targetPort: {{ .Values.containerPorts.http }}
      protocol: TCP
```

#### service-printer.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: {{ include "mes-server.fullname" . }}-printer
spec:
  type: {{ .Values.servicePrinter.type }}
  selector:
    {{- include "mes-server.selectorLabels" . | nindent 4 }}
  ports:
    - name: printer
      port: {{ .Values.servicePrinter.port }}
      targetPort: {{ .Values.containerPorts.printer }}
      nodePort: {{ .Values.servicePrinter.nodePort }}
      protocol: TCP
```

#### ingressroute.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: {{ include "mes-server.fullname" . }}-printer
spec:
  type: {{ .Values.servicePrinter.type }}
  selector:
    {{- include "mes-server.selectorLabels" . | nindent 4 }}
  ports:
    - name: printer
      port: {{ .Values.servicePrinter.port }}
      targetPort: {{ .Values.containerPorts.printer }}
      nodePort: {{ .Values.servicePrinter.nodePort }}
      protocol: TCP
```

#### middleware.yaml

```yaml
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: mes-server-rewrite-api
  namespace: mes
spec:
  replacePathRegex:
    regex: "^/(app|admin)-api/(.*)"
    replacement: "/$1-api/$2"   # 保留前端路径
```

## web部署

### CI部署部分

#### 项目镜像打包

```dockerfile
#拉取nginx镜像
FROM 	nginx:latest
#文件维护者
LABEL authors="wang"

#删除nginx默认配置
RUN rm /etc/nginx/nginx.conf

#将自定义的nginx配置文件复制到nginx目录下
COPY nginx.conf /etc/nginx/

#将构建好的项目文件复制到nginx目录下
COPY dist /usr/share/nginx/html/

EXPOSE 80

# 运行 nginx，使用前台模式，不要使用守护进程模式
CMD ["nginx", "-g", "daemon off;"]
```

#### Jenkinsfile流水线

```groovy
#!groovy
pipeline {
    agent any
    tools { nodejs 'node-v14.21.3' }
    environment {
        //声明全局变量，方便后使用
        harborUser = 'admin'
        harborPasswd = '123456'
        harborAddress = '192.168.1.166:80'
        harborRepo = 'mes-web'
    }

    stages {
        stage('检出') {
            steps {
                checkout scmGit(branches: [[name: '${tag}']], extensions: [], userRemoteConfigs: [[credentialsId: 'b1f83372-2a32-4a6c-bf34-549a268', url: 'http://192.168.1.142/root/mes-web.git']])
            }
        }

        stage('构建') {
            steps {
                sh 'npm  install'
                sh 'npm  run build:prod'
            }
        }
        stage('通过docker制作自定义镜像') {
            steps {
                sh '''rm -rf /var/jenkins_home/workspace/k8s-web/k8s/dist
             mv /var/jenkins_home/workspace/k8s-web/dist  /var/jenkins_home/workspace/k8s-web/k8s/
             docker build -t  dist:${tag} /var/jenkins_home/workspace/k8s-web/k8s'''
            }
        }
        stage('将自定义镜像推送到Harbor仓库') {
            steps {
                sh '''docker login -u ${harborUser} -p ${harborPasswd} ${harborAddress}
                   docker tag dist:${tag} 192.168.1.166:80/mes-web/dist:${tag}
                   docker push ${harborAddress}/${harborRepo}/dist:${tag}'''
            }
        }
        stage('更新 DevOps 仓库中的后端镜像版本') {
            steps {
                withCredentials([
                        usernamePassword(
                                credentialsId: 'devops-git-cred',
                                usernameVariable: 'GIT_USER',
                                passwordVariable: 'GIT_PASS'
                        )
                ]) {
                    script {
                        // 对密码进行 URL 编码
                        ENCODED_PASS = java.net.URLEncoder.encode(GIT_PASS, 'UTF-8')
                    }
                    sh """
                echo "===== 克隆 DevOps 仓库 ====="
                rm -rf devops-repo

                # 指定 Jenkins 的 HOME 目录
                export HOME=/var/jenkins_home

                # 删除旧的不属于 Jenkins 的文件
                rm -f \$HOME/.git-credentials

                # 写入 GitLab 凭证（使用编码后的密码）
                echo "http://${GIT_USER}:${ENCODED_PASS}@192.168.1.142" > \$HOME/.git-credentials
                chmod 600 \$HOME/.git-credentials

                git config --global credential.helper store

                # 测试一下 Git 是否能读取凭证
                git config --global --list

                git clone http://192.168.1.142/root/devops.git devops-repo
                cd devops-repo

                echo "===== 切换分支 ====="
                git checkout main

                echo "===== 修改 mes-web 的 Helm values.yaml ====="
                sed -i "s/tag: .*/tag: ${tag}/" ./charts/mes-web/values.yaml

                echo "===== 配置 Git 用户 ====="
                git config user.name "Administrator"
                git config user.email "admin@example.com"

                echo "===== 提交并 push ====="
                git add .
                git commit -m "Update mes-web image tag to ${tag}" || echo "No changes to commit"
                git push origin main
            """
                }
            }
        }
    }
    post {
        success {
            dingtalk(
                robot: 'web-k8s',
                type:'MARKDOWN',
                title: "success: ${JOB_NAME}",
                text: ["- 成功构建:${JOB_NAME}项目!\n- 版本:${tag}\n- 持续时间:${currentBuild.durationString}\n- 任务:#${JOB_NAME}"]
            )
        }
        failure {
            dingtalk(
                robot: 'web-k8s',
                type:'MARKDOWN',
                title: "fail: ${JOB_NAME}",
                text: ["- 失败构建:${JOB_NAME}项目!\n- 版本:${tag}\n- 持续时间:${currentBuild.durationString}\n- 任务:#${JOB_NAME}"]
            )
        }
    }
}
```

### CD部署部分

#### Application.yaml

```yaml
replicaCount: 3

image:
  repository: 192.168.1.166:80/mes-web/dist
  tag: v3.30.24
  pullPolicy: Always

service:
  type: ClusterIP
  port: 80

env:
  VUE_APP_API_BASE_URL: "https://kmes.abc.com"

ingress:
  enabled: true
  host: kmes.abc.com
  tlsSecretName: abc-com-tls
```

#### Chart.yaml

```yaml
apiVersion: v2
name: mes-web
description: MES 前端 Web 应用
type: application
version: 1.0.0
appVersion: "1.16.0"
```

#### values.yaml

```yaml
replicaCount: 3

image:
  repository: 192.168.1.166:80/mes-web/dist
  tag: v3.30.24
  pullPolicy: Always

service:
  type: ClusterIP
  port: 80

env:
  VUE_APP_API_BASE_URL: "https://kmes.abc.com"

ingress:
  enabled: true
  host: kmes.abc.com
  tlsSecretName: abc-com-tls
```

#### _helpers.tpl

```yaml
{{- define "mes-web.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" }}
{{- end }}

{{- define "mes-web.fullname" -}}
{{- if .Values.fullnameOverride }}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- $name := default .Chart.Name .Values.nameOverride }}
{{- if contains $name .Release.Name }}
{{- .Release.Name | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" }}
{{- end }}
{{- end }}
{{- end }}

{{- define "mes-web.chart" -}}
{{- printf "%s-%s" .Chart.Name .Chart.Version | trunc 63 | trimSuffix "-" }}
{{- end }}

{{- define "mes-web.labels" -}}
helm.sh/chart: {{ include "mes-web.chart" . }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}
{{- end }}

{{- define "mes-web.selectorLabels" -}}
app.kubernetes.io/name: {{ include "mes-web.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
{{- end }}
```

#### deployment.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "mes-web.fullname" . }}
  labels:
    {{- include "mes-web.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      {{- include "mes-web.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "mes-web.selectorLabels" . | nindent 8 }}
    spec:
      containers:
        - name: {{ .Chart.Name }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - containerPort: 80
          env:
            - name: VUE_APP_API_BASE_URL
              value: "{{ .Values.env.VUE_APP_API_BASE_URL }}"
```

#### ingressroute.yaml

```yaml
{{- if .Values.ingress.enabled }}
apiVersion: traefik.io/v1alpha1
kind: IngressRoute
metadata:
  name: {{ include "mes-web.fullname" . }}
  namespace: {{ .Release.Namespace }}
spec:
  entryPoints:
    - websecure
  routes:
    - kind: Rule
      match: Host(`{{ .Values.ingress.host }}`) && PathPrefix(`/`)
      priority: 1
      services:
        - name: {{ include "mes-web.fullname" . }}
          port: {{ .Values.service.port }}
  tls:
    secretName: {{ .Values.ingress.tlsSecretName }}
{{- end }}
```

#### service.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: {{ include "mes-web.fullname" . }}
  labels:
    {{- include "mes-web.labels" . | nindent 4 }}
spec:
  type: {{ .Values.service.type }}
  selector:
    {{- include "mes-web.selectorLabels" . | nindent 4 }}
  ports:
    - port: {{ .Values.service.port }}
      targetPort: 80
      protocol: TCP
```

## app部署

### CI部署部分

#### 项目打包

```dockerfile
#拉取nginx镜像
FROM 	nginx:latest
#文件维护者
LABEL authors="wang"

#删除nginx默认配置
RUN rm /etc/nginx/nginx.conf

#将自定义的nginx配置文件复制到nginx目录下
COPY nginx.conf /etc/nginx/

#将构建好的项目文件复制到nginx目录下
COPY h5 /usr/share/nginx/html/

EXPOSE 80

# 运行 nginx，使用前台模式，不要使用守护进程模式
CMD ["nginx", "-g", "daemon off;"]
```

#### Jenkinsfile流水线

```groovy
#!groovy
pipeline {
  agent any
  tools { nodejs 'node-v18.20.8' }

  environment {
    //仓库地址和密码
    harborUser = 'admin'
    harborPasswd = '123456'
    harborAddress = '192.168.1.166:80'
    harborRepo = 'mes-app'
    //关闭 Puppeteer 自动下载 Chromium 浏览器
    PUPPETEER_SKIP_DOWNLOAD = 'true'
    //设置淘宝镜像
    NPM_CONFIG_REGISTRY = 'https://registry.npmmirror.com'
  }

  stages {
    stage('检出') {
      steps {
        checkout scmGit(
          branches: [[name: "${params.tag}"]],
          extensions: [],
          userRemoteConfigs: [[
            credentialsId: 'b1f83372-2a32-4a6c-bf34-59a268',
            url: 'http://192.168.1.142/root/mes-app.git'
          ]]
        )
      }
    }

    stage('构建') {
      steps {
          // 临时加入 Node 路径
          sh 'export PATH=/home/jenkins_home/node-v18.20.8/bin:$PATH'
          //临时使用
          sh 'npm install -g yarn'
          // 设置淘宝镜像源（推荐放在构建前）
          sh 'yarn config set registry https://registry.npmmirror.com'

          // 安装依赖（带超时参数防止卡住）
          sh 'yarn install --network-timeout 100000'

          // 构建命令（按项目需要选择）
          sh 'yarn build:dev:h5'
      }
    }

    stage('通过docker制作自定义镜像') {
      steps {
        sh '''
          rm -rf /var/jenkins_home/workspace/k8s-app/k8s/h5
          mv /var/jenkins_home/workspace/k8s-app/dist/dev/h5 /var/jenkins_home/workspace/k8s-app/k8s/
          docker build -t h5:${tag} /var/jenkins_home/workspace/k8s-app/k8s/
        '''
      }
    }

    stage('将自定义镜像推送到Harbor仓库') {
      steps {
        sh """
          docker login -u ${harborUser} -p ${harborPasswd} ${harborAddress}
          docker tag h5:\${tag} ${harborAddress}/${harborRepo}/h5:\${tag}
          docker push ${harborAddress}/${harborRepo}/h5:\${tag}
        """
      }
    }

    stage('更新 DevOps 仓库中的后端镜像版本') {
      steps {
        withCredentials([
                        usernamePassword(
                                credentialsId: 'devops-git-cred',
                                usernameVariable: 'GIT_USER',
                                passwordVariable: 'GIT_PASS'
                        )
                ]) {
          script {
            // 对密码进行 URL 编码
            ENCODED_PASS = java.net.URLEncoder.encode(GIT_PASS, 'UTF-8')
          }
          sh """
                echo "===== 克隆 DevOps 仓库 ====="
                rm -rf devops-repo

                # 指定 Jenkins 的 HOME 目录
                export HOME=/var/jenkins_home

                # 删除旧的不属于 Jenkins 的文件
                rm -f \$HOME/.git-credentials

                # 写入 GitLab 凭证（使用编码后的密码）
                echo "http://${GIT_USER}:${ENCODED_PASS}@192.168.1.142" > \$HOME/.git-credentials
                chmod 600 \$HOME/.git-credentials

                git config --global credential.helper store

                # 测试一下 Git 是否能读取凭证
                git config --global --list

                git clone http://192.168.1.142/root/devops.git devops-repo
                cd devops-repo

                echo "===== 切换分支 ====="
                git checkout main

                echo "===== 修改 mes-app 的 Helm values.yaml ====="
                sed -i "s/tag: .*/tag: ${tag}/" ./charts/mes-app/values.yaml

                echo "===== 配置 Git 用户 ====="
                git config user.name "Administrator"
                git config user.email "admin@example.com"

                echo "===== 提交并 push ====="
                git add .
                git commit -m "Update mes-app image tag to ${tag}" || echo "No changes to commit"
                git push origin main
            """
                }
      }
    }
  }

  post {
    success {
      dingtalk(
        robot: 'app-k8s',
        type:'MARKDOWN',
        title: "success: ${JOB_NAME}",
        text: ["- 成功构建:${JOB_NAME}项目!\n- 版本:${params.tag}\n- 持续时间:${currentBuild.durationString}\n- 任务:#${JOB_NAME}"]
      )
    }
    failure {
      dingtalk(
        robot: 'app-k8s',
        type:'MARKDOWN',
        title: "fail: ${JOB_NAME}",
        text: ["- 失败构建:${JOB_NAME}项目!\n- 版本:${params.tag}\n- 持续时间:${currentBuild.durationString}\n- 任务:#${JOB_NAME}"]
      )
    }
  }
}
```

### CD部署部分

#### Application.yaml

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: mes-web
  namespace: argocd  # ArgoCD 安装的 namespace，通常是 argocd
spec:
  project: default

  source:
    repoURL: http://192.168.1.142/root/devops.git
    targetRevision: main
    path: charts/mes-web  # 你的 helm chart 路径
    helm:
      releaseName: mes-web
      valueFiles:
        - values.yaml

  destination:
    server: https://kubernetes.default.svc
    namespace: mes  # 你的应用部署到的 namespace

  syncPolicy:
    automated:
      prune: true          # 自动删除不需要的资源
      selfHeal: true       # 自动修复偏移
    syncOptions:
      - CreateNamespace=true
```

#### values.yaml

```yaml
replicaCount: 3

image:
  repository: 192.168.1.166:80/mes-app/h5
  tag: v3.29.26
  pullPolicy: Always

service:
  type: ClusterIP
  port: 80

resources:
  limits:
    cpu: "500m"
    memory: "512Mi"
  requests:
    cpu: "200m"
    memory: "256Mi"

env:
  VUE_APP_API_BASE_URL: "https://kapp.abc.com"

ingress:
  enabled: true
  host: kapp.abc.com
  tlsSecretName: abc-com-tls
```

#### Chart.yaml

```yaml
apiVersion: v2
name: mes-app
description: MES App 端
type: application
version: 0.1.0
appVersion: "1.0"
```

#### service.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: {{ include "mes-app.fullname" . }}
spec:
  type: {{ .Values.service.type }}
  selector:
    {{- include "mes-app.selectorLabels" . | nindent 4 }}
  ports:
    - port: {{ .Values.service.port }}
      targetPort: 80
      protocol: TCP
```

#### ingressroute.yaml

```yaml
{{- if .Values.ingress.enabled }}
apiVersion: traefik.io/v1alpha1
kind: IngressRoute
metadata:
  name: {{ include "mes-app.fullname" . }}
  namespace: {{ .Release.Namespace }}
spec:
  entryPoints:
    - websecure
  routes:
    - match: Host(`{{ .Values.ingress.host }}`)
      kind: Rule
      priority: 1
      services:
        - name: {{ include "mes-app.fullname" . }}
          port: {{ .Values.service.port }}
  tls:
    secretName: {{ .Values.ingress.tlsSecretName }}
{{- end }}
```

#### deployment.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "mes-app.fullname" . }}
  labels:
    {{- include "mes-app.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      {{- include "mes-app.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "mes-app.selectorLabels" . | nindent 8 }}
    spec:
      containers:
        - name: app
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - containerPort: 80
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
```

#### _helpers.tpl

```yaml
{{- define "mes-app.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" }}
{{- end }}

{{- define "mes-app.fullname" -}}
{{- if .Values.fullnameOverride }}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- $name := default .Chart.Name .Values.nameOverride }}
{{- if contains $name .Release.Name }}
{{- .Release.Name | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" }}
{{- end }}
{{- end }}
{{- end }}

{{- define "mes-app.chart" -}}
{{- printf "%s-%s" .Chart.Name .Chart.Version | trunc 63 | trimSuffix "-" }}
{{- end }}

{{- define "mes-app.labels" -}}
helm.sh/chart: {{ include "mes-app.chart" . }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}
{{- end }}

{{- define "mes-app.selectorLabels" -}}
app.kubernetes.io/name: {{ include "mes-app.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
{{- end }}
```

