---
icon: pen-to-square
date: 2025-12-19
category:
  - Linux
tag:
  - k8s
  - Kubernetes
  - HAProxy
  - Keepalived
star: true
sticky: true #标记
---
# 高可用运行原理

# 一句话先给结论（先立认知）

> **Kubernetes 高可用 ≠ 只有一个在干活**
>  而是：
>  👉 **多个控制面组件同时运行**
>  👉 **对外只有一个入口（VIP）**
>  👉 **内部靠选主 + etcd 一致性保证正确性**

------

# 一、你的真实架构图（文字版）

```
        kubectl / kubelet / controller
                    │
                    ▼
          ┌─────────────────────┐
          │   VIP 192.168.1.238│  ← Keepalived 漂移
          └─────────┬───────────┘
                    │
               HAProxy :6443
          ┌─────────┼───────────┐
          │          │           │
   master01      master02     master03
 kube-apiserver kube-apiserver kube-apiserver
          │          │           │
          └─────────┴───────────┘
                    │
               etcd 集群 (3)
```

------

# 二、VIP（Keepalived）在干什么？

### 核心一句话

> **VIP 只是“门牌号”，不干活**

### 你现在的情况

- VIP：`192.168.128.238`
- **只在 lb01 / lb02 上存在**
- master 节点 **永远不绑 VIP**

### Keepalived 的作用

- 同一时间只有 **一台 LB 持有 VIP**
- LB 挂了 → VIP 自动漂移
- 客户端永远访问同一个 IP

👉 **VIP = 高可用入口，不参与 K8s 逻辑**

------

# 三、HAProxy 在干什么？

### 核心一句话

> **HAProxy 只负责“转发”，不理解 Kubernetes**

你的配置：

```
frontend k8s-apiserver
    bind 192.168.1.238:6443

backend k8s-apiserver
    server master01 192.168.1.230:6443 check
    server master02 192.168.1.231:6443 check
    server master03 192.168.1.232:6443 check
```

### HAProxy 做的事情

1. 收到 `VIP:6443` 请求
2. 选一个 **存活的 apiserver**
3. TCP 转发过去
4. 完事

 ❌ 它 **不知道 leader**
 ❌ 不知道 etcd
 ❌ 不参与调度

👉 **HAProxy = 负载均衡 + 存活探测**

------

# 四、真正的“大脑”：kube-apiserver

### 重点认知（非常重要）

> **三个 kube-apiserver 全部同时工作**

不是主备！
不是只有一个！

### 那不会乱吗？

不会，原因在 **etcd**

------

# 五、etcd：高可用的“裁判 + 账本”

### etcd 是什么？

- 分布式一致性数据库（Raft）
- **所有集群状态只认 etcd**

```
Pod / Node / Service / ConfigMap / Lease
         ↓
       etcd
```

### etcd 高可用规则

- 3 个节点
- 1 个 leader
- 写操作必须大多数（2/3）同意

👉 **不会脑裂**

------

# 六、controller-manager / scheduler 怎么不冲突？

### 你看到的是这样：

```
kube-scheduler-master01   Running
kube-scheduler-master02   Running
kube-scheduler-master03   Running
```

### 实际运行机制：**Leader Election**

- 3 个实例全部启动
- **etcd 中有一个 Lease**
- 谁抢到 Lease，谁是 leader
- 只有 leader 真正“干活”

其他实例：

```
Running 但不执行逻辑
```

### Leader 挂了？

- Lease 超时
- 其他实例立刻抢
- **秒级切换**

👉 **这才是真正的 HA**

------

# 七、kubelet / kubectl 是怎么访问的？

### kubelet

每个节点的 kubelet：

```
--kubeconfig
server: https://192.168.1.238:6443
```

流程：

```
kubelet
  ↓
VIP
  ↓
HAProxy
  ↓
任意 kube-apiserver
  ↓
etcd
```

### kubectl

同理。

👉 **所有客户端永远只认 VIP**

------

# 八、为什么 apiserver 要 `--advertise-address=本机IP`？

你之前问过这个，非常关键：

```
--advertise-address=192.168.1.230
```

### 原因

- apiserver **注册到 etcd 的地址**
- 其他组件通过这个地址回连它
- **必须是自己真实 IP**

❌ 如果写 VIP：

- 所有 apiserver 都注册成同一个地址
- 回环 / 冲突 / 异常
- etcd & controller 都会乱

👉 **VIP 只给“客户端”，不给“组件间通信”**

------

# 九、故障时真实发生了什么？

### 假设 master01 宕机

| 层级      | 发生的事                       |
| --------- | ------------------------------ |
| apiserver | HAProxy 健康检查失败，不再转发 |
| etcd      | 2/3 存活，继续工作             |
| scheduler | 其他实例接管 leader            |
| kubelet   | 无感知                         |
| 用户      | 无感知                         |

### 假设 LB1 宕机

| 层级   | 发生的事     |
| ------ | ------------ |
| VIP    | 漂移到 LB2   |
| 客户端 | TCP 短暂重连 |
| 集群   | 正常         |

------

# 十、总结一句“架构级理解”

> **Kubernetes 高可用的本质是：**
>
> - 入口高可用（VIP）
> - 控制面多实例
> - 状态集中在 etcd
> - 行为靠 Lease 选主
> - 客户端永远无感知



# **为什么要这样设计**

# 一句话核心答案（先给结论）

> **Kubernetes 这样设计，是为了在“不可靠的机器、不可靠的网络、不可靠的人”上，提供“尽可能可靠的系统”。**

所有设计，都是为了解决这 5 个“无法回避的问题”。

------

# 一、为什么一定要 VIP + 负载均衡？

### 问题 1：IP 是不可靠的

- 任意一台 master 都可能：
  - 宕机
  - 重启
  - 网络抖动
- 但 **客户端不能知道这些**

### 如果没有 VIP 会怎样？

❌ kubelet 直接连 master01
 → master01 挂了
 → 全集群瘫痪

### 所以设计成：

```
客户端 → 稳定地址（VIP）
```

👉 **稳定入口 ≠ 稳定节点**

------

# 二、为什么 kube-apiserver 要多实例？

### 问题 2：单点必然会挂

你现在 master01 能跑：

- 明天磁盘满
- 后天内核 panic
- 下周人误删

### 如果只有 1 个 apiserver

→ **控制面全死**

### 所以设计为：

- kube-apiserver：**无状态**
- 可以无限横向扩展
- 任意挂一个都无所谓

👉 **所有状态外置**

------

# 三、为什么所有状态都放 etcd？

### 问题 3：状态必须是“唯一事实”

在分布式系统里：

> **只能有一个“真相来源”**

如果：

- scheduler 认为 Pod 已调度
- controller 认为还没调度

→ 集群必乱

### etcd 的价值

- 强一致（Raft）
- 多数派写入
- 不会脑裂

👉 **etcd = 全集群唯一裁判**

------

# 四、为什么 controller / scheduler 也要多实例但只干一个？

### 问题 4：你既要高可用，又不能重复执行

例子：

- 两个 scheduler 同时调度一个 Pod
- 两个 controller 同时删资源

→ **灾难**

### 所以 Kubernetes 的选择是：

```
多实例 + 单 leader
```

通过：

- etcd Lease
- TTL 自动过期

实现：

- leader 死亡 → 自动接管
- 无需人工干预

👉 **这是分布式系统最成熟的模式**

------

# 五、为什么不用“主备切换”那一套？

你可能会想：

> 直接主备不行吗？

### 传统主备的问题

| 问题     | 后果               |
| -------- | ------------------ |
| 切换慢   | 分钟级             |
| 人工介入 | 容易出错           |
| 脑裂     | 主备都以为自己是主 |

### Kubernetes 的选择

- 所有实例同时在线
- 没有“冷备”
- **随时可接管**

👉 **这是为“云环境”设计的，不是为机房设计的**

------

# 六、为什么 apiserver 不直接选 leader？

这是个**非常关键的问题**。

### 因为 apiserver 是“入口层”

- 它不应该有“主”
- 它只做：
  - 校验
  - 鉴权
  - 写入 etcd

如果 apiserver 自己选主：

- 会变复杂
- 会变慢
- 会引入更多失败点

👉 **把复杂度下沉到 etcd**

------

# 七、为什么设计成“组件都很傻”？

你可能注意到了：

- HAProxy 很傻
- apiserver 很傻
- kubelet 很傻

### 原因：**简单才能可靠**

> 分布式系统里：
>
> **越聪明的组件，越容易出 bug**

K8s 的哲学：

- 组件简单
- 规则清晰
- 状态集中
- 行为可恢复

👉 **出问题能自愈**

------

# 八、为什么不用数据库 / MySQL？

你是做 MES 的，这个点你一定会共鸣 👇

### MySQL 的问题

- 主从切换复杂
- 网络分区容易写丢
- 脑裂难解决

### etcd 的优势

- 强一致
- 明确多数派
- 明确失败语义

👉 **K8s 要的是“宁可不可用，也不能错”**

------

# 九、为什么这些设计在小集群看起来很“复杂”？

因为你现在是：

> **用“企业级分布式系统”解决“小规模问题”**

但 Kubernetes 的目标是：

- 100 节点
- 1000 节点
- 10000 节点

👉 **复杂度是为规模买单**

------

# 十、一句终极总结（架构级）

> Kubernetes 的高可用设计，本质是在贯彻三条铁律：
>
> 1️⃣ **任何东西都会挂**
> 2️⃣ **网络永远不可靠**
> 3️⃣ **状态必须唯一**

读到这里，已经具备了：

- 分布式系统直觉
- 架构设计能力
- 高可用系统认知