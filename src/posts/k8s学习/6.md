# RKE2安装

## 中文文档地址

https://docs.rancher.cn/docs/rke2/install/quickstart/_index

## GitHub地址

https://github.com/rancher/rke2/

## 国内镜像网站

https://docker.aityp.com/

## 修改时区

```sh
sudo timedatectl set-timezone Asia/Shanghai
# 查看是否成功
timedatectl
```

## 设置时间同步(小集群)

```sh
#更新包
sudo apt update
#安装
sudo apt install systemd-timesyncd -y
#开机自启
sudo systemctl enable systemd-timesyncd --now
#改成阿里云 NTP
sudo nano /etc/systemd/timesyncd.conf
#加上配置
[Time]
NTP=ntp.aliyun.com
FallbackNTP=ntp1.aliyun.com ntp2.aliyun.com ntp.tencent.com time.pool.aliyun.com
#重启
sudo systemctl restart systemd-timesyncd
#查看状态
timedatectl status
```

## 用`chrony`(适合大规模集群，更稳)

```sh
#更新包
sudo apt update
#安装
sudo apt install chrony -y
#修改配置文件
sudo vim /etc/chrony/chrony.conf
#填写阿里的时间服务器
server ntp.aliyun.com iburst
server ntp1.aliyun.com iburst
server ntp2.aliyun.com iburst
#系统启动自动启动
sudo systemctl enable chrony --now
#强制同步
sudo chronyc makestep
#查看同步状态
chronyc sources -v
```

## 设置主机名

```sh
#编辑以下文件
sudo vim  /etc/hosts
192.168.128.230 masters
192.168.128.231 node01
192.168.128.232 node02
```

## mastrs节点安装

> 下载离线包

```sh
https://github.com/rancher/rke2/tags
```

### 下载安装脚本

```sh
sudo curl -sfL https://rancher-mirror.rancher.cn/rke2/install.sh
sudo INSTALL_RKE2_MIRROR=cn INSTALL_RKE2_VERSION="v1.32.8+rke2r1" sh install.sh  #指定v1.32.8+rke2r1版本，cn是环境

```

### 将离线包放到这目录下

```sh
sudo mkdir -p /var/lib/rancher/rke2/agent/images/
sudo mv rke2-images.linux-amd64.tar.zst /var/lib/rancher/rke2/agent/images/
```

### 启动

```sh
sudo systemctl restart rke2-server
```

### 查看服务状态

```sh
sudo systemctl status rke2-server -l
sudo journalctl -u rke2-server -f
```

### 验证集群是否成功启动

```sh
# RKE2 会在 /etc/rancher/rke2/rke2.yaml 生成 kubeconfig：
sudo cat /etc/rancher/rke2/rke2.yaml
# 然后你就可以用 kubectl（默认安装在 /var/lib/rancher/rke2/bin/kubectl）测试：
export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
kubectl get nodes
kubectl get pods -A
```

### 设置环境变量,方便直接使用kubectl

```sh
echo 'export PATH=$PATH:/var/lib/rancher/rke2/bin' | sudo tee -a /etc/profile.d/rke2.sh
source /etc/profile.d/rke2.sh
#给权限
sudo chmod 644 /etc/rancher/rke2/rke2.yaml
#默认
echo 'export KUBECONFIG=/etc/rancher/rke2/rke2.yaml' >> ~/.bashrc
source ~/.bashrc
#验证
kubectl get nodes
```

### 启动日志查看

```sh
sudo journalctl -u rke2-server -f
```

## worker节点

### 下载安装脚本

```sh
sudo curl -sfL https://rancher-mirror.rancher.cn/rke2/install.sh -o install.sh
# INSTALL_RKE2_TYPE="agent" 增加这个配置，就不会安装控制组件
sudo INSTALL_RKE2_MIRROR=cn INSTALL_RKE2_TYPE="agent" INSTALL_RKE2_VERSION="v1.32.8+rke2r1" sh install.sh
```

### 下载离线镜像，并将包放到指定目录下

```sh
sudo mkdir -p /var/lib/rancher/rke2/agent/images/
sudo mv rke2-images.linux-amd64.tar.zst /var/lib/rancher/rke2/agent/images/
```

### 配置 rke2-agent 服务

```sh
mkdir -p /etc/rancher/rke2/
vim /etc/rancher/rke2/config.yaml
```

### 填写内容

>本地也必须用 https://<master-ip>:9345，因为 RKE2 的 server 端默认就是用 TLS 起的 9345 端口。

```sh
server: https://masters:9345 
token: K104ae9acd991afd22246e01e5080b711a4198f4d92f3b858ebb1db99b8a6b2f7ed::server:548edec63f78e4f97e7295f43d64de63
node-name: node01
```

### 获取token

```sh
sudo cat /var/lib/rancher/rke2/server/node-token
```

### 开机自启

```sh
sudo systemctl enable rke2-agent.service
```

### 启动

```sh
sudo systemctl start rke2-agent
```

### 查看启动日志

```sh
sudo journalctl -u rke2-agent -f
```

## 安装 cert-manager

> **HTTPS 证书的申请、续签、分发都交给 cert-manager 来处理**
> **cert-manager 自动处理 ACME 协议**，通过 Ingress (nginx) 完成域名验证，然后把证书存到 Secret 里。**邮箱**：在 `ClusterIssuer` 里配置，用于向 Let’s Encrypt 注册账号，**也能接收证书过期提醒邮件**。
>
> **证书有效期**：Let’s Encrypt 默认 90 天，cert-manager 会在过期前 30 天自动续签。
>
> **自动化程度**：证书创建 → 绑定 Ingress → 自动续签，全部无人值守。

```yaml
#离线下载，根据自己的版本下载
https://github.com/helm/helm/tags
# 1. 安装 CRDs（必须先安装）
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.4/cert-manager.crds.yaml
#离线安装
kubectl apply -f cert-manager.crds.yaml
# 2. 添加 Jetstack 仓库
helm repo add jetstack https://charts.jetstack.io
helm repo update

# 3. 安装 cert-manager
helm install cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --version v1.18.2
```

## 桌面管理软件

https://k8slens.dev/

## 安装rancher管理界面(WEB)

### 官网(文档)

https://ranchermanager.docs.rancher.com/zh/

### 国内镜像网站

https://docker.aityp.com/

### 安装helm

`版本下载`：https://github.com/helm/helm/tags

```sh
wget https://get.helm.sh/helm-v3.18.6-linux-amd64.tar.gz
tar -zxvf helm-v3.18.6-linux-amd64.tar.gz
sudo mv linux-amd64/helm /usr/local/bin/helm
helm version
```

### 添加仓库：

```sh
helm repo add rancher-stable https://rancher-mirror.rancher.cn/server-charts/stable
helm repo updatehelm repo add stable https://charts.helm.sh/stable
helm repo update
```

### 安装rancher(NodePort方式连接)

```sh
helm install rancher rancher-stable/rancher \
  --namespace cattle-system \
  --create-namespace \
  --set hostname=rancher.local \
  --set replicas=1 \
  --set bootstrapPassword=admin \
  --set ingress.enabled=false \
  --set service.type=NodePort \
  --set rancherImage=registry.cn-hangzhou.aliyuncs.com/rancher/rancher \
  --set rancherImageTag=v2.11.3 \
  --set rancherShellImage=rancher/shell \
  --set rancherShellImageTag=v0.4.1
  -set rancherShellImage=rancher/rancher-webhook \
  --set rancherShellImageTag=v0.7.3
```

> `-set ingress.enabled=false \ #禁用 Ingress，不创建 Ingress 资源。`
>
> `--set rancherImage=registry.cn-hangzhou.aliyuncs.com/rancher/rancher \ #用阿里云的镜像`
>
> `--set bootstrapPassword=admin \ #默认密码`
>
> `安装前先用一台能连通docker的系统，下载rancher/shell和rancher/rancher-webhook 操作请看下面：`

### 用docker下载指定容器

```sh
docker pull rancher/shell:v0.4.1
sudo docker save rancher/shell:v0.4.1 > rancher-shell-v0.4.1.tar
#将文件传给目标文件
scp rancher-shell-v0.4.1.tar mes@192.168.128.230:/tmp/
#导入容器中
sudo /var/lib/rancher/rke2/bin/ctr \
  --address /run/k3s/containerd/containerd.sock \
  -n k8s.io images import rancher-shell-v0.4.1.tar
#查看导入是否成功
sudo /var/lib/rancher/rke2/bin/ctr \
  --address /run/k3s/containerd/containerd.sock \
  -n k8s.io images ls | grep shell
#改别名
sudo /var/lib/rancher/rke2/bin/ctr \
  --address /run/k3s/containerd/containerd.sock \
  -n k8s.io images tag docker.io/rancher/shell:v0.4.1 rancher/shell:v0.4.1
```

### 创建values.yaml

```yaml
# values.yaml
hostname: rancher.local

replicas: 1

bootstrapPassword: 62352744aa??aa

ingress:
  enabled: false

service:
  type: NodePort

rancherImage: registry.cn-hangzhou.aliyuncs.com/rancher/rancher
rancherImageTag: v2.11.3

rancherShellImage: rancher/shell
rancherShellImageTag: v0.4.1


rancherWebhookImage: rancher/rancher-webhook
rancherWebhookImageTag: v0.7.3
```

### 运行文件

```sh
helm install rancher rancher-stable/rancher \
  --namespace cattle-system \
  --create-namespace \
  -f values.yaml
  #更新
  helm upgrade rancher rancher-stable/rancher \
  -n cattle-system \
  -f values.yaml
  #强制覆盖
  helm upgrade rancher rancher-stable/rancher \
  -n cattle-system \
  -f values.yaml \
  --force --reset-values
```

### 添加加速镜像

```json
mirrors:
  "docker.io":
    endpoint:
      - "https://cf-workers-docker-io-784.pages.dev"
      - "https://registry.cn-hangzhou.aliyuncs.com"
      - "https://docker.m.daocloud.io"
configs:
  "docker.io":
    auth:
      username: ""
      password: ""
```

> `mirrors`：把 `docker.io` 的镜像请求重定向到国内镜像源。
>
> `configs`：可选，填入私有仓库认证信息（如果有）。

### 拉取镜像

```sh
sudo /var/lib/rancher/rke2/bin/ctr -n k8s.io images pull registry.cn-hangzhou.aliyuncs.com/rancher/shell:v0.4.1
sudo /var/lib/rancher/rke2/bin/ctr -n k8s.io images tag registry.cn-hangzhou.aliyuncs.com/rancher/shell:v0.4.1 rancher/shell:v0.4.1

```

### helm命令使用

```sh
#查看某个库的chart
helm search repo rancher-stable
#更新仓库
helm repo update
#删除部署
helm uninstall rancher -n cattle-system

```

### kubectl命令使用

```sh
# 确认 Pod 已经清理干净
kubectl get pods -n cattle-system
#强制删掉 Job 和 Pod
kubectl delete job rancher-post-delete -n cattle-system --force --grace-period=0
kubectl delete pod rancher-post-delete-nbhc7 -n cattle-system --force --grace-period=0
#查看命名空间信息
kubectl get all -n cattle-system
#查找残留的 release Secret
kubectl get secret -n cattle-system | grep rancher
#删除 Secret
kubectl delete secret -n cattle-system sh.helm.release.v1.rancher.v1
kubectl delete secret -n cattle-system sh.helm.release.v1.rancher.v2
#跳过 Hook 强制卸载
helm uninstall rancher -n cattle-system --no-hooks
sudo helm uninstall rancher -n cattle-system --no-hooks --kubeconfig /etc/rancher/rke2/rke2.yaml
#删除残留
kubectl delete secret tls-rancher tls-rancher-internal tls-rancher-internal-ca -n cattle-system
#清理残留资源
kubectl delete all --all -n cattle-system
kubectl delete jobs --all -n cattle-system
#确认 namespace 干净
kubectl get all -n cattle-system
#重启pod
kubectl -n cattle-system delete pod -l app=rancher
#查看pod信息
kubectl get pods -n cattle-system -o wide
```

### 获取权限

```sh
mkdir -p ~/.kube
sudo cp /etc/rancher/rke2/rke2.yaml ~/.kube/config
sudo chown $(id -u):$(id -g) ~/.kube/config
sudo chmod 600 ~/.kube/config
#添加到环境
sudo vim ~/.bashrc
#加到最后面
export KUBECONFIG=$HOME/.kube/config
#刷新
source ~/.bashrc
```

## 安装Argo CD

### 下载安装文件

```sh
https://raw.githubusercontent.com/argoproj/argo-cd/v3.0.6/manifests/install.yaml
#修改容器源地址
quay.io/argoproj/argocd
#替换成
swr.cn-north-4.myhuaweicloud.com/ddn-k8s/quay.io/argoproj/argocd:v3.0.6
```

### 安装运行

```sh
#创建命名空间
kubectl create namespace argocd
#运行命令
kubectl apply -n argocd -f ArgoCD-install.yaml
#删除
kubectl delete -n argocd -f ArgoCD-install.yaml
```

### 获取密码

```sh
#取初始 admin 密码（登录之后记得改密码）
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d && echo
```

## 搭建 NFS 服务器

```sh
# 以 Ubuntu 为例
sudo apt update
sudo apt install -y nfs-kernel-server

# 创建导出目录
sudo mkdir -p /data/nfs
sudo chown -R mes:mes /data/nfs
sudo chmod -R 777 /data/nfs

# 配置导出
echo "/data/nfs *(rw,sync,no_subtree_check,no_root_squash)" | sudo tee -a /etc/exports

# 生效配置
sudo exportfs -rav

#验证是否正常
showmount -e <NFS服务器IP>

#需要在工作节点上安装客户端
sudo apt-get update
sudo apt-get install -y nfs-common
```

### master节点安装

```sh
#用浏览器安装
https://raw.githubusercontent.com/kubernetes-sigs/nfs-subdir-external-provisioner/master/deploy/rbac.yaml
```

> 这个 `rbac.yaml` 是 NFS Subdir External Provisioner 用来申请权限（RBAC，Role-Based Access Control）的一段配置。它定义了 ServiceAccount、ClusterRole、Role、ClusterRoleBinding、RoleBinding 等资源，以便 provisioner 有权限执行必要的操作。

### 运行

```sh
#运行权限
kubectl apply -f rbac.yaml
```

### 给操作权限

```yaml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
  namespace: kube-system
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  namespace: kube-system
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  namespace: kube-system
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    namespace: kube-system
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io

```

### 编写deployment.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      containers:
        - name: nfs-client-provisioner
          image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: "nk8s-sigs.io/nfs-subdir-external-provisioner"
            - name: NFS_SERVER
              value: "192.168.128.233"
            - name: NFS_PATH
              value: "/data/nfs"
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.128.233
            path: /data/nfs

```

### 创建一个 StorageClass

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-storage   # 存储类的名字，PVC 申请存储时会引用它
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner  # 这里指定了存储供应者（Provisioner）
parameters:
  archiveOnDelete: "false" # PVC 删除时，是否保留数据（false 表示直接删掉，不归档）
reclaimPolicy: Retain  # PVC 删除时 PV 的回收策略，Retain 表示保留，不自动回收
mountOptions:
  - vers=4.1   # 挂载参数，这里指定 NFS 使用 v4.1 协议
```

### 创建PVC测试案例

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
spec:
  accessModes:
    - ReadWriteMany     # 多个 Pod 可以同时挂载
  resources:
    requests:
      storage: 1Gi      # 申请 1G 存储
  storageClassName: nfs-storage   # 要和你 StorageClass 的名字一致
```

### StorageClass、PVC、PV关系图

```perl
         ┌─────────────────────┐
         │   StorageClass      │
         │  (nfs-storage)      │
         │  Provisioner=NFS    │
         └─────────┬───────────┘
                   │
    开发者下单 (申请存储)
                   │
         ┌─────────▼───────────┐
         │   PVC (声明需求)     │
         │  requests: 1Gi       │
         │  accessModes: RWX    │
         │  storageClassName=   │
         │     nfs-storage      │
         └─────────┬───────────┘
                   │
   Provisioner 根据 StorageClass 创建真实存储
                   │
         ┌─────────▼───────────┐
         │   PV (实际资源)      │
         │ capacity: 1Gi        │
         │ accessModes: RWX     │
         │ nfs: /data/nfs/...   │
         └─────────┬───────────┘
                   │
     PVC 绑定 PV，Pod 通过 PVC 使用存储
                   │
         ┌─────────▼───────────┐
         │   Pod (应用)         │
         │  volumes:            │
         │    - claimName=PVC   │
         │  挂载到 /mnt/data    │
         └─────────────────────┘
```

### 执行

```sh
#创建
kubectl apply -f test-pvc.yaml
#查看状态
kubectl get pvc test-pvc
#删掉建新的
kubectl delete storageclass nfs-storage
```

## 安装MetalLB(负载均衡)

### 官网

https://metallb.io/

### 添加仓库helm

```sh
helm repo add metallb https://metallb.github.io/metallb
helm repo update
```

### 安装

```sh
# 安装 metallb 到 metallb-system 命名空间
helm install metallb metallb/metallb -n metallb-system --create-namespace
```

### 配置 IPAddressPool

> 新建一个 `IPAddressPool`，告诉 MetalLB 可以用哪些 IP 来分配给 `Service type=LoadBalancer`

```yaml
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: default-address-pool
  namespace: metallb-system
spec:
  addresses:
  - 192.168.1.240-192.168.1.250 #分配的IP，分配之后这些IP不能被其他设备使用
```

### 配置 L2Advertisement

> 让 MetalLB 广播这段 IP

```yaml
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: default
  namespace: metallb-system
spec:
  ipAddressPools:
  - default-address-pool
```

### 验证是否安装成功

```yaml
apiVersion: v1
kind: Service
metadata:
  name: test-nginx
spec:
  type: LoadBalancer
  selector:
    app: nginx
  ports:
    - port: 80
      targetPort: 80
```

### 查看IP是否分配成功

```sh
kubectl get svc test-nginx
```

### 成功分配

```sh
mes@masters:~$ kubectl get svc test-nginx
NAME         TYPE           CLUSTER-IP    EXTERNAL-IP       PORT(S)        AGE
test-nginx   LoadBalancer   10.43.70.81   192.168.128.234   80:31710/TCP   33s
```

### 删除测试

```sh
kubectl delete -f test-nginx.yaml
```

## 应用部署(springboot+vue)

### 部署的流程图

```perl
   ┌──────────────┐
      │   Developer  │
      │   (写代码)   │
      └───────┬──────┘
              │ push
              ▼
      ┌──────────────┐
      │   GitLab     │
      │(源码仓库/CI) │
      └───────┬──────┘
              │ Webhook 触发
              ▼
      ┌──────────────┐
      │   Jenkins    │
      │(CI 构建流水线)│
      └───────┬──────┘
              │ 1. 构建 SpringBoot Jar / Vue 打包
              │ 2. docker build
              │ 3. push image
              ▼
      ┌──────────────┐
      │   Harbor     │
      │(镜像仓库)    │
      └───────┬──────┘
              │ 镜像 tag 更新
              ▼
      ┌──────────────┐
      │ GitOps Repo  │
      │ (ArgoCD 管理)│
      │ apps/        │
      │  ├─ springboot-service.yaml
      │  ├─ vue-frontend.yaml
      │  └─ ...      │
      │ charts/      │
      │  └─ springboot-chart/values-dev.yaml │
      └───────┬──────┘
              │ Jenkins 自动修改 values 文件 (更新 image tag) 并 push
              ▼
      ┌──────────────┐
      │   Argo CD    │
      │ App-of-Apps  │
      │ root-app.yaml│
      └───────┬──────┘
              │ 自动同步 GitOps Repo
              ▼
      ┌──────────────┐
      │ Kubernetes   │
      │   Cluster    │
      │  ├─ Pod (SpringBoot) │
      │  ├─ Pod (Vue)        │
      │  ├─ Ingress / Service│
      │  └─ PVC / NFS        │
      └──────────────┘
```

